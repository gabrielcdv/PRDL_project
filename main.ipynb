{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a9c3a4",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ab3b8",
   "metadata": {},
   "source": [
    "# Downloading data (Traffic and Air quality). \n",
    "Automatically downloads from the following dates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_from = datetime(2024, 2, 1)  # Example: January 2023\n",
    "download_until = datetime(2024, 2, 28) # Or set a specific end date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff831cb",
   "metadata": {},
   "source": [
    "Then run the following to actually download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed67fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwonloading files...\n",
      "File already exists: data/TRAMS_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_02_Febrer.csv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Catalan month names\n",
    "catalan_months = {\n",
    "    1: \"Gener\",\n",
    "    2: \"Febrer\",\n",
    "    3: \"Marc\",\n",
    "    4: \"Abril\",\n",
    "    5: \"Maig\",\n",
    "    6: \"Juny\",\n",
    "    7: \"Juliol\",\n",
    "    8: \"Agost\",\n",
    "    9: \"Setembre\",\n",
    "    10: \"Octubre\",\n",
    "    11: \"Novembre\",\n",
    "    12: \"Desembre\",\n",
    "}\n",
    "\n",
    "# Trams transit relacio (relation between ids and locations)\n",
    "trams_relacio_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/1090983a-1c40-4609-8620-14ad49aae3ab/resource/1d6c814c-70ef-4147-aa16-a49ddb952f72/download/transit_relacio_trams.csv\"\n",
    "trams_relacio_path = \"./data/transit_relacio_trams.csv\"\n",
    "\n",
    "# Air quality stations info (including lat and long)\n",
    "# (The stations are unchanged since 2023 so downloading only the 2025 version is enough)\n",
    "air_stations_info_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/4dff88b1-151b-48db-91c2-45007cd5d07a/resource/d1aa40d7-66f9-451b-85f8-955b765fdc2f/download/2025_qualitat_aire_estacions.csv\"\n",
    "air_stations_info_path = \"./data/air_stations_info.csv\"\n",
    "\n",
    "def generate_urls(download_from, download_until):\n",
    "    urls = []\n",
    "    current_date = datetime(download_from.year, download_from.month, 1)\n",
    "    end_date = datetime(download_until.year, download_until.month, 1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        month_name = catalan_months[month]\n",
    "\n",
    "        # Generate URLs for both datasets\n",
    "        tram_url = f\"https://opendata-ajuntament.barcelona.cat/resources/auto/transit/{year}_{month:02d}_{month_name}_TRAMS_TRAMS.csv\"\n",
    "        aire_url = f\"https://opendata-ajuntament.barcelona.cat/resources/bcn/QualitatAire/{year}_{month:02d}_{month_name}_qualitat_aire_BCN.csv\"\n",
    "\n",
    "        tram_filename = f\"data/TRAMS_{year}_{month:02d}_{month_name}.csv\"\n",
    "        aire_filename = f\"data/QualitatAire_{year}_{month:02d}_{month_name}.csv\"\n",
    "\n",
    "        urls.append((tram_url, tram_filename))\n",
    "        urls.append((aire_url, aire_filename))\n",
    "\n",
    "        # Move to the next month\n",
    "        if current_date.month == 12:\n",
    "            current_date = datetime(current_date.year + 1, 1, 1)\n",
    "        else:\n",
    "            current_date = datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Create a \"data\" folder:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "urls = generate_urls(download_from, download_until)\n",
    "\n",
    "print(\"Dwonloading files...\")\n",
    "for url, filename in urls:\n",
    "    download_file(url, filename)\n",
    "\n",
    "\n",
    "# Then download the TRAMS relacio file\n",
    "if not os.path.exists(trams_relacio_path):\n",
    "    download_file(trams_relacio_url, trams_relacio_path)\n",
    "\n",
    "\n",
    "# And the air stations info \n",
    "if not os.path.exists(air_stations_info_path):\n",
    "    download_file(air_stations_info_url, air_stations_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aec697",
   "metadata": {},
   "source": [
    "# Data treatment and cleaning\n",
    "\n",
    "\n",
    "## Creating a merged dataset\n",
    "\n",
    "### Combining all the air quality files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e437d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "58               8  Barcelona             19  Barcelona        4   \n",
      "59               8  Barcelona             19  Barcelona        4   \n",
      "60               8  Barcelona             19  Barcelona        4   \n",
      "61               8  Barcelona             19  Barcelona        4   \n",
      "62               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "    CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H20  V20   H21  V21   H22  \\\n",
      "58                10  2024    2    1  23.0  ...  47.0    V  44.0    V  38.0   \n",
      "59                10  2024    2    2  24.0  ...  46.0    V  32.0    V  30.0   \n",
      "60                10  2024    2    3  21.0  ...  23.0    V  24.0    V  29.0   \n",
      "61                10  2024    2    4  30.0  ...  27.0    V  30.0    V  37.0   \n",
      "62                10  2024    2    5  27.0  ...  33.0    V  26.0    V  30.0   \n",
      "\n",
      "    V22   H23  V23   H24  V24  \n",
      "58    V  21.0    V  22.0    V  \n",
      "59    V  33.0    V  25.0    V  \n",
      "60    V  29.0    V  32.0    V  \n",
      "61    V  37.0    V  36.0    V  \n",
      "62    V  31.0    V  32.0    V  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" ## Now merge the air_quality measurements and the location\\n\\nair_quality = air_quality_combined.merge(\\n    right=air_station_locations,\\n    left_on='ESTACIO',\\n    right_on='Estacio',\\n    how='left'\\n).drop(columns=['Estacio'])\\n\\nprint(air_quality.head()) \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "air_quality_files = [filename for url, filename in urls if \"QualitatAire\" in filename]\n",
    "#print(air_quality_files)\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "air_quality_dfs = [pd.read_csv(file) for file in air_quality_files]\n",
    "# Combine them into a single dataframe\n",
    "air_quality = pd.concat(air_quality_dfs, ignore_index=True)\n",
    "#print(air_quality_combined.head())\n",
    "\n",
    "\n",
    "\n",
    "# Only keep PM10 DATA\n",
    "air_quality = air_quality[air_quality[\"CODI_CONTAMINANT\"].isin([10, 110])]\n",
    "print(air_quality.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "air_station_info = pd.read_csv(air_stations_info_path)\n",
    "# Only keep location data for each station\n",
    "air_station_locations = air_station_info[['Estacio', 'Latitud', 'Longitud']].drop_duplicates(subset=['Estacio'])\n",
    "#print(air_station_locations.head(n=20))\n",
    "\n",
    "\"\"\" ## Now merge the air_quality measurements and the location\n",
    "\n",
    "air_quality = air_quality_combined.merge(\n",
    "    right=air_station_locations,\n",
    "    left_on='ESTACIO',\n",
    "    right_on='Estacio',\n",
    "    how='left'\n",
    ").drop(columns=['Estacio'])\n",
    "\n",
    "print(air_quality.head()) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d460",
   "metadata": {},
   "source": [
    "### Combining traffic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9908b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual  estatPrevist\n",
      "0       1  20240201102053            2             0\n",
      "1       2  20240201102053            2             0\n",
      "2       3  20240201102053            2             0\n",
      "3       4  20240201102053            2             0\n",
      "4       5  20240201102053            1             0\n",
      "RESULT FINA\n",
      "   idTram            data  estatActual       lat        lon\n",
      "0       1  20240201102053            2  2.106769  41.382911\n",
      "1       2  20240201102053            2  2.106769  41.383167\n",
      "2       3  20240201102053            2  2.117372  41.385579\n",
      "3       4  20240201102053            2  2.117281  41.385824\n",
      "4       5  20240201102053            1  2.125109  41.387561\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "traffic_files = [filename for url, filename in urls if \"TRAMS\" in filename]\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "traffic_dfs = [pd.read_csv(file) for file in traffic_files]\n",
    "#print(traffic_dfs[0].head())\n",
    "\n",
    "# Combine them into a single dataframe\n",
    "traffic_combined = pd.concat(traffic_dfs, ignore_index=True)\n",
    "print(traffic_combined.head())\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "trams_info = pd.read_csv(trams_relacio_path)\n",
    "# Only keep location data for each station\n",
    "trams_locations = trams_info[['Tram', 'Coordenades']]\n",
    "\n",
    "# Only keep one location point per section (tram=section)\n",
    "def mean_coordinate(row):\n",
    "    # print(\"entering function mean\")\n",
    "    # print(row)\n",
    "    coord_text=row['Coordenades']\n",
    "    numbers=coord_text.split(',')\n",
    "    assert len(numbers) % 2 == 0\n",
    "    lats = [float(x) for x in numbers[::2]]\n",
    "    lons = [float(x) for x in numbers[1::2]]\n",
    "    # print(\"infos\")\n",
    "    # print(lats)\n",
    "    # print(type(lats[0]))\n",
    "    lat = np.mean(lats)\n",
    "    lon = np.mean(lons)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "trams_locations [['lat' , 'lon']] = trams_locations.apply(mean_coordinate, axis=1)\n",
    "#print(trams_locations.head())\n",
    "\n",
    "\n",
    "## Now merge the traffic measurements and the location\n",
    "traffic = traffic_combined.merge(\n",
    "    right=trams_locations,\n",
    "    left_on='idTram',\n",
    "    right_on='Tram',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "traffic = traffic.drop(['Coordenades', 'Tram', 'estatPrevist'], axis=1)\n",
    "\n",
    "print(\"RESULT FINA\")\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364615f",
   "metadata": {},
   "source": [
    "## Dataset issue\n",
    "\n",
    "For some reason, the city does not provide information for the 'trams' with id greater than 527. In the `traffic` dataframe, there are some records with idTram between 535 and 539. They don't have a latitude/longitude. Let us drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b751bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = traffic.dropna(subset=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6d93",
   "metadata": {},
   "source": [
    "# Defining a traffic grid\n",
    "\n",
    "Now, `traffic` contains the state (from 0-no car to 6-congestioned) of plenty of coordinates in Barcelona. But the density is not homogeneous, hence i will cut the city in a grid and compute an average value for traffic congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eb04e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual       lat        lon  zone\n",
      "0       1  20240201102053            2  2.106769  41.382911     2\n",
      "1       2  20240201102053            2  2.106769  41.383167     2\n",
      "2       3  20240201102053            2  2.117372  41.385579     3\n",
      "3       4  20240201102053            2  2.117281  41.385824     3\n",
      "4       5  20240201102053            1  2.125109  41.387561    11\n",
      "    CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "58               8  Barcelona             19  Barcelona        4   \n",
      "59               8  Barcelona             19  Barcelona        4   \n",
      "60               8  Barcelona             19  Barcelona        4   \n",
      "61               8  Barcelona             19  Barcelona        4   \n",
      "62               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "    CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H20  V20   H21  V21   H22  \\\n",
      "58                10  2024    2    1  23.0  ...  47.0    V  44.0    V  38.0   \n",
      "59                10  2024    2    2  24.0  ...  46.0    V  32.0    V  30.0   \n",
      "60                10  2024    2    3  21.0  ...  23.0    V  24.0    V  29.0   \n",
      "61                10  2024    2    4  30.0  ...  27.0    V  30.0    V  37.0   \n",
      "62                10  2024    2    5  27.0  ...  33.0    V  26.0    V  30.0   \n",
      "\n",
      "    V22   H23  V23   H24  V24  \n",
      "58    V  21.0    V  22.0    V  \n",
      "59    V  33.0    V  25.0    V  \n",
      "60    V  29.0    V  32.0    V  \n",
      "61    V  37.0    V  36.0    V  \n",
      "62    V  31.0    V  32.0    V  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# Getting the boundaries of the traffic information\n",
    "minlat = traffic['lat'].min()\n",
    "maxlat = traffic['lat'].max()\n",
    "minlon = traffic['lon'].min()\n",
    "maxlon = traffic['lon'].max()\n",
    "\n",
    "#TODO add prints of the distance of the box here\n",
    "\n",
    "\n",
    "nb_horizontal = 8\n",
    "nb_vertical = 8\n",
    "\n",
    "vertical_step = (maxlat - minlat) / nb_vertical\n",
    "horizontal_step = (maxlon - minlon) / nb_horizontal\n",
    "\n",
    "\n",
    "# Create new datafram from traffic\n",
    "\n",
    "\"\"\" def getZone(row):\n",
    "    lat = (row['lat'] - minlat) // vertical_step\n",
    "    lon = (row['lon'] - minlon) // horizontal_step\n",
    "    zone_number = (lat * nb_horizontal) + lon\n",
    "\n",
    "\n",
    "traffic['zone'] = traffic.apply(getZone, axis=1) \"\"\"\n",
    "# BETTER WAY (vectorized)\n",
    "# Create nb_vertical * nb_horizontal zones and assign every row a zone:\n",
    "lat_idx = np.clip(np.floor((traffic['lat'] - minlat) / vertical_step).astype(int), 0, nb_vertical - 1)\n",
    "lon_idx = np.clip(np.floor((traffic['lon'] - minlon) / horizontal_step).astype(int), 0, nb_horizontal - 1)\n",
    "traffic['zone'] = lat_idx * nb_horizontal + lon_idx\n",
    "\n",
    "assert traffic['zone'].max() <= nb_horizontal * nb_vertical\n",
    "\n",
    "# Drop lat/lon as we will now use the zone\n",
    "traffic.drop(['lat', 'lon'], axis=1)\n",
    "\n",
    "\n",
    "print(traffic.head())\n",
    "print(air_quality.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ecc1a",
   "metadata": {},
   "source": [
    "# Merging all the data\n",
    "We are then going to merge both of the dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615fd52",
   "metadata": {},
   "source": [
    "## Modification of the `traffic` dataframe\n",
    "Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour. Then we will melt the dataframe so that [hour, date] is a primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88fdc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone        date  hour  0  2  3  4  10  11  12  13  ...  47  51  52  53  54  \\\n",
      "0     2024-02-01    10  0  2  2  1   1   1   2   2  ...   1   2   0   1   0   \n",
      "1     2024-02-01    11  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "2     2024-02-01    12  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "3     2024-02-01    13  0  2  2  1   2   2   2   2  ...   1   2   0   1   0   \n",
      "4     2024-02-01    14  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "\n",
      "zone  55  60  61  62  63  \n",
      "0      0   0   0   2   2  \n",
      "1      0   0   0   2   2  \n",
      "2      0   0   0   2   2  \n",
      "3      0   0   0   2   2  \n",
      "4      0   0   0   2   3  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Covnert the 'data' (date) column of traffic dataframe\n",
    "traffic[\"datetime\"] = pd.to_datetime(traffic[\"data\"], format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour \n",
    "traffic[\"datetime_hour\"] = traffic[\"datetime\"].dt.round(\"h\")\n",
    "traffic[\"date\"] = traffic[\"datetime_hour\"].dt.date\n",
    "traffic[\"hour\"] = traffic[\"datetime_hour\"].dt.hour\n",
    "\n",
    "\n",
    "# Melting\n",
    "traffic = traffic.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['zone'],\n",
    "    values='estatActual',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "print(traffic.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bf4c1",
   "metadata": {},
   "source": [
    "## Modification of the `air_quality` dataframe\n",
    "The hours are currently in the columns, we are going to melt the dataframe to have a long one with dates in the rows.\n",
    "\n",
    "We also want [hour,date] to be a primary key so that merging both dataframes does not explodes RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b4a5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  hour  station_4  station_43  station_44  station_54  \\\n",
      "0  2024-02-01     1       23.0        30.0        40.0         NaN   \n",
      "1  2024-02-01     2       19.0        45.0        35.0         NaN   \n",
      "2  2024-02-01     3       19.0        45.0        34.0         NaN   \n",
      "3  2024-02-01     4       19.0        41.0        32.0         NaN   \n",
      "4  2024-02-01     5       19.0        36.0        29.0         NaN   \n",
      "\n",
      "   station_57  station_58  \n",
      "0        20.0        13.0  \n",
      "1        20.0        12.0  \n",
      "2        19.0        10.0  \n",
      "3        24.0        15.0  \n",
      "4        24.0        12.0  \n"
     ]
    }
   ],
   "source": [
    "# Get the hours columns name\n",
    "hour_cols = [col for col in air_quality.columns if col.startswith(\"H\") and len(col) == 3]\n",
    "\n",
    "# Then use pandas melt method to reshape the DF\n",
    "\n",
    "aq_melted = air_quality.melt(\n",
    "    # Columns left untouched\n",
    "    id_vars=[\"CODI_PROVINCIA\", \"PROVINCIA\", \"CODI_MUNICIPI\", \"MUNICIPI\", \"ESTACIO\", \"ANY\", \"MES\", \"DIA\"], # IMPORTANT: i removed CODI_CONTAMINANT because i only kept PM10 temporarily\n",
    "\n",
    "    #Columns to unpivot\n",
    "    value_vars=hour_cols,\n",
    "\n",
    "    #Name of the new column that will store the unpivoted columns names (H01 etc.)\n",
    "    var_name=\"hour_str\",\n",
    "\n",
    "    #Name of the actual value column\n",
    "    value_name=\"pollution_value\"\n",
    ")\n",
    "    \n",
    "# Clean the hour (H01->1)\n",
    "aq_melted[\"hour\"] = aq_melted[\"hour_str\"].str.extract(r\"H(\\d{2})\").astype(\"int32\")\n",
    "\n",
    "\n",
    "\n",
    "# Create the actual date column (and translate column names)\n",
    "aq_melted[\"date\"] = pd.to_datetime(\n",
    "    aq_melted[[\"ANY\", \"MES\", \"DIA\"]].rename(columns={\"ANY\": \"year\", \"MES\": \"month\", \"DIA\": \"day\"})\n",
    ").dt.date\n",
    "\n",
    "#Shift the H24 columns to hour 0 of the next day:\n",
    "mask_24 = aq_melted[\"hour\"] == 24\n",
    "aq_melted.loc[mask_24, \"hour\"] = 0\n",
    "aq_melted.loc[mask_24, \"date\"] = pd.to_datetime(aq_melted.loc[mask_24, \"date\"]) + pd.Timedelta(days=1)\n",
    "aq_melted[\"date\"] = pd.to_datetime(aq_melted[\"date\"]).dt.date\n",
    "\n",
    "\n",
    "# Pivot (CODI_CONTAMINANT,ESTACIO) so that [hour, date] is primary key\n",
    "aq_pivoted = aq_melted.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['ESTACIO'], # IMPORTANT: here i also remove CODI_CONTAMINANT since there is only one.\n",
    "    values='pollution_value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Simple flattening (for more meaningful column names)\n",
    "\"\"\" aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'pollutant_{col[0]}_station_{col[1]}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "] \"\"\"\n",
    "aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'station_{col}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "]\n",
    "\n",
    "print(aq_pivoted.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c08b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe `traffic`:\n",
      "zone        date  hour  0  2  3  4  10  11  12  13  ...  47  51  52  53  54  \\\n",
      "0     2024-02-01    10  0  2  2  1   1   1   2   2  ...   1   2   0   1   0   \n",
      "1     2024-02-01    11  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "2     2024-02-01    12  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "3     2024-02-01    13  0  2  2  1   2   2   2   2  ...   1   2   0   1   0   \n",
      "4     2024-02-01    14  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "\n",
      "zone  55  60  61  62  63  \n",
      "0      0   0   0   2   2  \n",
      "1      0   0   0   2   2  \n",
      "2      0   0   0   2   2  \n",
      "3      0   0   0   2   2  \n",
      "4      0   0   0   2   3  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "Dataframe aq: \n",
      "         date  hour  station_4  station_43  station_44  station_54  \\\n",
      "0  2024-02-01     1       23.0        30.0        40.0         NaN   \n",
      "1  2024-02-01     2       19.0        45.0        35.0         NaN   \n",
      "2  2024-02-01     3       19.0        45.0        34.0         NaN   \n",
      "3  2024-02-01     4       19.0        41.0        32.0         NaN   \n",
      "4  2024-02-01     5       19.0        36.0        29.0         NaN   \n",
      "\n",
      "   station_57  station_58  \n",
      "0        20.0        13.0  \n",
      "1        20.0        12.0  \n",
      "2        19.0        10.0  \n",
      "3        24.0        15.0  \n",
      "4        24.0        12.0  \n",
      "Final DF\n",
      "          date  hour  0  2  3  4  10  11  12  13  ...  60  61  62  63  \\\n",
      "0   2024-02-01    10  0  2  2  1   1   1   2   2  ...   0   0   2   2   \n",
      "1   2024-02-01    11  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "2   2024-02-01    12  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "3   2024-02-01    13  0  2  2  1   2   2   2   2  ...   0   0   2   2   \n",
      "4   2024-02-01    14  0  2  2  1   1   2   2   2  ...   0   0   2   3   \n",
      "5   2024-02-01    15  0  2  2  2   1   2   2   4  ...   0   0   2   2   \n",
      "6   2024-02-01    16  0  2  2  2   1   2   2   2  ...   0   0   2   2   \n",
      "7   2024-02-01    17  0  2  2  2   1   2   2   2  ...   0   0   2   2   \n",
      "8   2024-02-01    18  0  4  2  2   1   2   2   4  ...   0   0   2   2   \n",
      "9   2024-02-01    19  0  3  2  2   1   5   2   4  ...   0   0   2   3   \n",
      "10  2024-02-02    11  0  0  2  0   0   0   0   2  ...   0   0   0   2   \n",
      "11  2024-02-02    12  0  2  2  1   1   2   0   2  ...   0   0   2   2   \n",
      "12  2024-02-02    13  0  2  2  1   1   2   0   2  ...   0   0   2   2   \n",
      "13  2024-02-02    14  0  2  2  1   1   2   0   2  ...   0   0   2   3   \n",
      "14  2024-02-02    15  0  2  2  2   1   4   0   3  ...   0   0   2   4   \n",
      "15  2024-02-02    16  0  2  2  2   2   2   0   4  ...   0   0   2   4   \n",
      "16  2024-02-02    17  0  2  2  3   1   2   0   4  ...   0   0   2   3   \n",
      "17  2024-02-02    18  0  2  2  2   1   2   0   4  ...   0   0   2   2   \n",
      "18  2024-02-02    19  0  2  2  3   1   2   0   3  ...   0   0   2   2   \n",
      "19  2024-02-02    20  0  2  2  2   0   2   0   2  ...   0   0   2   2   \n",
      "20  2024-02-02    21  0  2  2  1   0   2   0   2  ...   0   0   2   4   \n",
      "21  2024-02-02    22  0  2  0  1   0   2   0   2  ...   0   0   2   2   \n",
      "22  2024-02-02    23  0  2  0  1   0   1   0   2  ...   0   0   1   2   \n",
      "23  2024-02-03     0  0  1  0  0   1   1   0   1  ...   0   0   1   2   \n",
      "24  2024-02-03     1  0  1  0  0   0   0   0   1  ...   0   0   1   2   \n",
      "25  2024-02-03     2  0  0  0  0   0   0   0   1  ...   0   0   0   1   \n",
      "26  2024-02-03     4  0  0  0  0   0   0   0   1  ...   0   0   0   1   \n",
      "27  2024-02-03     5  0  0  0  0   0   0   0   1  ...   0   0   0   1   \n",
      "28  2024-02-03     6  0  0  0  0   0   0   0   1  ...   0   0   0   1   \n",
      "29  2024-02-03     7  0  1  2  0   0   1   2   2  ...   0   0   0   2   \n",
      "30  2024-02-03     8  0  1  2  0   0   1   2   2  ...   0   0   1   2   \n",
      "31  2024-02-03     9  0  2  2  0   0   1   2   2  ...   0   0   2   1   \n",
      "32  2024-02-03    10  0  2  2  1   0   2   2   2  ...   0   0   2   2   \n",
      "33  2024-02-03    11  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "34  2024-02-03    12  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "35  2024-02-03    13  0  2  2  1   1   2   2   2  ...   0   0   2   3   \n",
      "36  2024-02-03    14  0  2  2  1   1   2   2   3  ...   0   0   2   3   \n",
      "37  2024-02-03    15  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "38  2024-02-03    16  0  2  2  1   0   2   2   2  ...   0   0   2   2   \n",
      "39  2024-02-03    17  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "40  2024-02-03    18  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "41  2024-02-03    19  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "42  2024-02-03    20  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "43  2024-02-03    21  0  2  2  1   0   2   2   2  ...   0   0   2   3   \n",
      "44  2024-02-03    22  0  2  2  1   0   2   2   2  ...   0   0   2   2   \n",
      "45  2024-02-03    23  0  1  2  0   0   1   2   2  ...   0   0   2   2   \n",
      "46  2024-02-04     0  0  1  2  0   0   1   2   1  ...   0   0   1   2   \n",
      "47  2024-02-04     1  0  1  2  0   0   1   2   1  ...   0   0   1   2   \n",
      "48  2024-02-04     2  0  1  2  0   0   0   2   1  ...   0   0   1   1   \n",
      "49  2024-02-04     3  0  0  2  0   0   0   2   1  ...   0   0   1   1   \n",
      "50  2024-02-04     5  0  0  2  0   0   0   2   1  ...   0   0   0   1   \n",
      "51  2024-02-04     6  0  0  2  0   0   0   2   1  ...   0   0   0   1   \n",
      "52  2024-02-04     7  0  0  2  0   0   0   2   1  ...   0   0   0   1   \n",
      "53  2024-02-04     8  0  1  2  0   0   1   2   1  ...   0   0   0   1   \n",
      "54  2024-02-04     9  0  1  2  0   0   0   2   1  ...   0   0   2   1   \n",
      "55  2024-02-04    10  0  1  2  1   0   1   2   1  ...   0   0   1   2   \n",
      "56  2024-02-04    11  0  2  2  1   1   1   2   2  ...   0   0   2   2   \n",
      "57  2024-02-04    12  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "58  2024-02-04    13  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "59  2024-02-04    14  0  2  2  1   1   2   2   2  ...   0   0   2   2   \n",
      "\n",
      "    station_4  station_43  station_44  station_54  station_57  station_58  \n",
      "0        51.0        69.0        57.0         NaN        45.0         NaN  \n",
      "1        51.0        58.0        57.0         NaN        25.0         NaN  \n",
      "2        51.0        51.0        48.0         NaN        34.0         NaN  \n",
      "3        49.0        46.0        41.0         NaN        24.0        16.0  \n",
      "4        44.0        46.0        35.0         NaN        31.0        17.0  \n",
      "5        35.0        47.0        45.0         NaN        31.0        21.0  \n",
      "6        54.0        44.0        37.0         NaN        28.0        31.0  \n",
      "7        57.0        59.0        39.0         NaN        32.0        26.0  \n",
      "8        82.0        67.0        50.0         NaN        39.0        23.0  \n",
      "9        51.0        64.0        54.0         NaN        36.0        23.0  \n",
      "10       51.0        55.0        40.0         NaN         NaN        23.0  \n",
      "11       41.0        50.0        42.0        55.0         NaN        20.0  \n",
      "12       34.0        39.0        41.0        43.0         NaN        19.0  \n",
      "13       34.0        38.0        38.0        37.0         NaN        24.0  \n",
      "14       34.0        37.0        33.0        28.0        19.0        19.0  \n",
      "15       42.0        48.0        36.0        31.0        17.0        22.0  \n",
      "16       59.0        64.0        53.0        33.0        26.0        27.0  \n",
      "17       95.0        71.0        59.0        33.0        30.0        23.0  \n",
      "18       63.0        64.0        56.0        33.0        35.0        19.0  \n",
      "19       46.0        55.0        39.0        42.0        25.0        18.0  \n",
      "20       32.0        48.0        50.0        45.0        20.0        21.0  \n",
      "21       30.0        51.0        39.0        45.0        19.0        19.0  \n",
      "22       33.0        55.0        43.0        33.0        21.0        16.0  \n",
      "23       25.0        58.0        58.0        22.0        20.0        15.0  \n",
      "24       21.0        48.0        44.0        21.0        21.0        15.0  \n",
      "25       20.0        35.0        38.0        21.0        21.0        14.0  \n",
      "26       18.0        36.0        38.0        16.0        19.0        13.0  \n",
      "27       19.0        45.0        37.0        17.0        17.0        15.0  \n",
      "28       20.0        31.0        30.0        18.0        16.0        13.0  \n",
      "29       20.0        28.0        29.0        15.0        16.0        13.0  \n",
      "30       26.0        33.0        38.0        14.0        15.0        14.0  \n",
      "31       24.0        41.0        47.0        16.0        15.0        11.0  \n",
      "32       31.0        40.0        41.0        18.0        17.0        22.0  \n",
      "33       42.0        43.0        31.0        21.0        19.0        17.0  \n",
      "34       48.0        38.0        15.0        19.0        17.0        12.0  \n",
      "35       42.0        42.0        21.0        15.0        12.0        12.0  \n",
      "36       49.0        37.0        34.0        31.0        24.0        17.0  \n",
      "37       34.0        30.0        36.0        43.0        19.0        33.0  \n",
      "38       21.0        31.0        26.0        35.0        17.0        25.0  \n",
      "39       29.0        26.0        33.0        29.0        15.0        23.0  \n",
      "40       27.0        24.0        25.0        40.0        16.0        18.0  \n",
      "41       25.0        25.0        24.0        43.0        18.0        16.0  \n",
      "42       23.0        27.0        29.0        40.0        19.0        13.0  \n",
      "43       24.0        24.0        28.0        38.0        19.0        16.0  \n",
      "44       29.0        32.0        31.0        24.0        20.0        11.0  \n",
      "45       29.0        39.0        28.0        16.0        21.0         9.0  \n",
      "46       32.0        39.0        19.0        22.0        22.0         6.0  \n",
      "47       30.0        37.0        23.0        15.0        12.0         5.0  \n",
      "48       26.0        51.0        38.0         9.0         8.0         7.0  \n",
      "49       17.0        29.0        28.0        10.0         9.0         7.0  \n",
      "50       13.0        22.0        25.0        11.0         9.0        10.0  \n",
      "51       13.0        26.0        23.0        12.0        10.0         4.0  \n",
      "52       14.0        29.0        20.0        13.0        10.0        10.0  \n",
      "53       15.0        30.0        32.0        13.0         9.0         5.0  \n",
      "54       16.0        40.0        42.0        13.0        11.0         8.0  \n",
      "55       19.0        38.0        59.0        21.0        14.0         7.0  \n",
      "56       23.0        43.0        36.0        21.0        19.0        27.0  \n",
      "57       32.0        37.0        31.0        23.0        19.0        24.0  \n",
      "58       38.0        42.0        38.0        25.0        15.0        36.0  \n",
      "59       42.0        43.0        49.0        44.0        19.0        38.0  \n",
      "\n",
      "[60 rows x 52 columns]\n",
      "taux de valeurs pourries\n",
      "0.0031390954528830914\n"
     ]
    }
   ],
   "source": [
    "# Now do the actual merging based on time\n",
    "print(\"Dataframe `traffic`:\")\n",
    "print(traffic.head())\n",
    "print(\"Dataframe aq: \")\n",
    "print(aq_pivoted.head())\n",
    "merged = pd.merge(\n",
    "    traffic,\n",
    "    aq_pivoted,\n",
    "    how=\"left\",\n",
    "    left_on=[\"date\", \"hour\"],\n",
    "    right_on=[\"date\", \"hour\"]\n",
    ")\n",
    "\n",
    "# is_primary_key = not traffic.duplicated(subset=['date', 'hour']).any()\n",
    "# print(f\"Is [date, hour] a primary key? {is_primary_key}\")\n",
    "\n",
    "print(\"Final DF\")\n",
    "print(merged.head(60))\n",
    "\n",
    "print(\"taux de valeurs pourries\")\n",
    "print(merged.isna().sum().sum() / merged.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6c1a1",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- nettoyer air quality: enlever les colonnes de polluants qui ne sont pas dans la référence(https://opendata-ajuntament.barcelona.cat/data/dataset/6960936a-95ed-4cc4-a6ec-e089197ccd8b/resource/c122329d-d26d-469e-bf9e-8efa10e4c127/download/qualitat_aire_contaminants.csv)\n",
    "dans la référence, les polluants avec une * sont plus précis.\n",
    "\n",
    "\n",
    "- se concentrer sur un seul/2 polluant (PM10=10, PM2.5=9)\n",
    "\n",
    "- Gérer les données vides\n",
    "\n",
    "\n",
    "\n",
    "We will focus first only on PM10 pollutant, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
