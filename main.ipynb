{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a9c3a4",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec0af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ab3b8",
   "metadata": {},
   "source": [
    "# Downloading data (Traffic and Air quality). \n",
    "Automatically downloads from the following dates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd2bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_from = datetime(2024, 2, 1)  # Example: January 2023\n",
    "download_until = datetime(2024, 2, 28) # Or set a specific end date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff831cb",
   "metadata": {},
   "source": [
    "Then run the following to actually download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed67fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwonloading files...\n",
      "File already exists: data/TRAMS_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_02_Febrer.csv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Catalan month names\n",
    "catalan_months = {\n",
    "    1: \"Gener\",\n",
    "    2: \"Febrer\",\n",
    "    3: \"Marc\",\n",
    "    4: \"Abril\",\n",
    "    5: \"Maig\",\n",
    "    6: \"Juny\",\n",
    "    7: \"Juliol\",\n",
    "    8: \"Agost\",\n",
    "    9: \"Setembre\",\n",
    "    10: \"Octubre\",\n",
    "    11: \"Novembre\",\n",
    "    12: \"Desembre\",\n",
    "}\n",
    "\n",
    "# Trams transit relacio (relation between ids and locations)\n",
    "trams_relacio_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/1090983a-1c40-4609-8620-14ad49aae3ab/resource/1d6c814c-70ef-4147-aa16-a49ddb952f72/download/transit_relacio_trams.csv\"\n",
    "trams_relacio_path = \"./data/transit_relacio_trams.csv\"\n",
    "\n",
    "# Air quality stations info (including lat and long)\n",
    "# (The stations are unchanged since 2023 so downloading only the 2025 version is enough)\n",
    "air_stations_info_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/4dff88b1-151b-48db-91c2-45007cd5d07a/resource/d1aa40d7-66f9-451b-85f8-955b765fdc2f/download/2025_qualitat_aire_estacions.csv\"\n",
    "air_stations_info_path = \"./data/air_stations_info.csv\"\n",
    "\n",
    "def generate_urls(download_from, download_until):\n",
    "    urls = []\n",
    "    current_date = datetime(download_from.year, download_from.month, 1)\n",
    "    end_date = datetime(download_until.year, download_until.month, 1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        month_name = catalan_months[month]\n",
    "\n",
    "        # Generate URLs for both datasets\n",
    "        tram_url = f\"https://opendata-ajuntament.barcelona.cat/resources/auto/transit/{year}_{month:02d}_{month_name}_TRAMS_TRAMS.csv\"\n",
    "        aire_url = f\"https://opendata-ajuntament.barcelona.cat/resources/bcn/QualitatAire/{year}_{month:02d}_{month_name}_qualitat_aire_BCN.csv\"\n",
    "\n",
    "        tram_filename = f\"data/TRAMS_{year}_{month:02d}_{month_name}.csv\"\n",
    "        aire_filename = f\"data/QualitatAire_{year}_{month:02d}_{month_name}.csv\"\n",
    "\n",
    "        urls.append((tram_url, tram_filename))\n",
    "        urls.append((aire_url, aire_filename))\n",
    "\n",
    "        # Move to the next month\n",
    "        if current_date.month == 12:\n",
    "            current_date = datetime(current_date.year + 1, 1, 1)\n",
    "        else:\n",
    "            current_date = datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Create a \"data\" folder:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "urls = generate_urls(download_from, download_until)\n",
    "\n",
    "print(\"Dwonloading files...\")\n",
    "for url, filename in urls:\n",
    "    download_file(url, filename)\n",
    "\n",
    "\n",
    "# Then download the TRAMS relacio file\n",
    "if not os.path.exists(trams_relacio_path):\n",
    "    download_file(trams_relacio_url, trams_relacio_path)\n",
    "\n",
    "\n",
    "# And the air stations info \n",
    "if not os.path.exists(air_stations_info_path):\n",
    "    download_file(air_stations_info_url, air_stations_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aec697",
   "metadata": {},
   "source": [
    "# Data treatment and cleaning\n",
    "\n",
    "\n",
    "## Creating a merged dataset\n",
    "\n",
    "### Combining all the air quality files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e437d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ## Now merge the air_quality measurements and the location\\n\\nair_quality = air_quality_combined.merge(\\n    right=air_station_locations,\\n    left_on='ESTACIO',\\n    right_on='Estacio',\\n    how='left'\\n).drop(columns=['Estacio'])\\n\\nprint(air_quality.head()) \""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "air_quality_files = [filename for url, filename in urls if \"QualitatAire\" in filename]\n",
    "#print(air_quality_files)\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "air_quality_dfs = [pd.read_csv(file) for file in air_quality_files]\n",
    "# Combine them into a single dataframe\n",
    "air_quality = pd.concat(air_quality_dfs, ignore_index=True)\n",
    "#print(air_quality_combined.head())\n",
    "\n",
    "\n",
    "\n",
    "# Only keep PM10 DATA\n",
    "air_quality = air_quality[air_quality[\"CODI_CONTAMINANT\"].isin([10, 110])]\n",
    "\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "air_station_info = pd.read_csv(air_stations_info_path)\n",
    "# Only keep location data for each station\n",
    "air_station_locations = air_station_info[['Estacio', 'Latitud', 'Longitud']].drop_duplicates(subset=['Estacio'])\n",
    "#print(air_station_locations.head(n=20))\n",
    "\n",
    "\"\"\" ## Now merge the air_quality measurements and the location\n",
    "\n",
    "air_quality = air_quality_combined.merge(\n",
    "    right=air_station_locations,\n",
    "    left_on='ESTACIO',\n",
    "    right_on='Estacio',\n",
    "    how='left'\n",
    ").drop(columns=['Estacio'])\n",
    "\n",
    "print(air_quality.head()) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d460",
   "metadata": {},
   "source": [
    "### Combining traffic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9908b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual  estatPrevist\n",
      "0       1  20240201102053            2             0\n",
      "1       2  20240201102053            2             0\n",
      "2       3  20240201102053            2             0\n",
      "3       4  20240201102053            2             0\n",
      "4       5  20240201102053            1             0\n",
      "RESULT FINA\n",
      "   idTram            data  estatActual       lat        lon\n",
      "0       1  20240201102053            2  2.106769  41.382911\n",
      "1       2  20240201102053            2  2.106769  41.383167\n",
      "2       3  20240201102053            2  2.117372  41.385579\n",
      "3       4  20240201102053            2  2.117281  41.385824\n",
      "4       5  20240201102053            1  2.125109  41.387561\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "traffic_files = [filename for url, filename in urls if \"TRAMS\" in filename]\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "traffic_dfs = [pd.read_csv(file) for file in traffic_files]\n",
    "#print(traffic_dfs[0].head())\n",
    "\n",
    "# Combine them into a single dataframe\n",
    "traffic_combined = pd.concat(traffic_dfs, ignore_index=True)\n",
    "print(traffic_combined.head())\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "trams_info = pd.read_csv(trams_relacio_path)\n",
    "# Only keep location data for each station\n",
    "trams_locations = trams_info[['Tram', 'Coordenades']]\n",
    "\n",
    "# Only keep one location point per section (tram=section)\n",
    "def mean_coordinate(row):\n",
    "    # print(\"entering function mean\")\n",
    "    # print(row)\n",
    "    coord_text=row['Coordenades']\n",
    "    numbers=coord_text.split(',')\n",
    "    assert len(numbers) % 2 == 0\n",
    "    lats = [float(x) for x in numbers[::2]]\n",
    "    lons = [float(x) for x in numbers[1::2]]\n",
    "    # print(\"infos\")\n",
    "    # print(lats)\n",
    "    # print(type(lats[0]))\n",
    "    lat = np.mean(lats)\n",
    "    lon = np.mean(lons)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "trams_locations [['lat' , 'lon']] = trams_locations.apply(mean_coordinate, axis=1)\n",
    "#print(trams_locations.head())\n",
    "\n",
    "\n",
    "## Now merge the traffic measurements and the location\n",
    "traffic = traffic_combined.merge(\n",
    "    right=trams_locations,\n",
    "    left_on='idTram',\n",
    "    right_on='Tram',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "traffic = traffic.drop(['Coordenades', 'Tram', 'estatPrevist'], axis=1)\n",
    "\n",
    "print(\"RESULT FINA\")\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364615f",
   "metadata": {},
   "source": [
    "## Dataset issue\n",
    "\n",
    "For some reason, the city does not provide information for the 'trams' with id greater than 527. In the `traffic` dataframe, there are some records with idTram between 535 and 539. They don't have a latitude/longitude. Let us drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b751bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = traffic.dropna(subset=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6d93",
   "metadata": {},
   "source": [
    "# Defining a traffic grid\n",
    "\n",
    "Now, `traffic` contains the state (from 0-no car to 6-congestioned) of plenty of coordinates in Barcelona. But the density is not homogeneous, hence i will cut the city in a grid and compute an average value for traffic congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3eb04e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual       lat        lon  zone\n",
      "0       1  20240201102053            2  2.106769  41.382911     2\n",
      "1       2  20240201102053            2  2.106769  41.383167     2\n",
      "2       3  20240201102053            2  2.117372  41.385579     3\n",
      "3       4  20240201102053            2  2.117281  41.385824     3\n",
      "4       5  20240201102053            1  2.125109  41.387561    11\n",
      "    CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "58               8  Barcelona             19  Barcelona        4   \n",
      "59               8  Barcelona             19  Barcelona        4   \n",
      "60               8  Barcelona             19  Barcelona        4   \n",
      "61               8  Barcelona             19  Barcelona        4   \n",
      "62               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "    CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H20  V20   H21  V21   H22  \\\n",
      "58                10  2024    2    1  23.0  ...  47.0    V  44.0    V  38.0   \n",
      "59                10  2024    2    2  24.0  ...  46.0    V  32.0    V  30.0   \n",
      "60                10  2024    2    3  21.0  ...  23.0    V  24.0    V  29.0   \n",
      "61                10  2024    2    4  30.0  ...  27.0    V  30.0    V  37.0   \n",
      "62                10  2024    2    5  27.0  ...  33.0    V  26.0    V  30.0   \n",
      "\n",
      "    V22   H23  V23   H24  V24  \n",
      "58    V  21.0    V  22.0    V  \n",
      "59    V  33.0    V  25.0    V  \n",
      "60    V  29.0    V  32.0    V  \n",
      "61    V  37.0    V  36.0    V  \n",
      "62    V  31.0    V  32.0    V  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# Getting the boundaries of the traffic information\n",
    "minlat = traffic['lat'].min()\n",
    "maxlat = traffic['lat'].max()\n",
    "minlon = traffic['lon'].min()\n",
    "maxlon = traffic['lon'].max()\n",
    "\n",
    "#TODO add prints of the distance of the box here\n",
    "\n",
    "\n",
    "nb_horizontal = 8\n",
    "nb_vertical = 8\n",
    "\n",
    "vertical_step = (maxlat - minlat) / nb_vertical\n",
    "horizontal_step = (maxlon - minlon) / nb_horizontal\n",
    "\n",
    "\n",
    "# Create new datafram from traffic\n",
    "\n",
    "\"\"\" def getZone(row):\n",
    "    lat = (row['lat'] - minlat) // vertical_step\n",
    "    lon = (row['lon'] - minlon) // horizontal_step\n",
    "    zone_number = (lat * nb_horizontal) + lon\n",
    "\n",
    "\n",
    "traffic['zone'] = traffic.apply(getZone, axis=1) \"\"\"\n",
    "# BETTER WAY (vectorized)\n",
    "# Create nb_vertical * nb_horizontal zones and assign every row a zone:\n",
    "lat_idx = np.clip(np.floor((traffic['lat'] - minlat) / vertical_step).astype(int), 0, nb_vertical - 1)\n",
    "lon_idx = np.clip(np.floor((traffic['lon'] - minlon) / horizontal_step).astype(int), 0, nb_horizontal - 1)\n",
    "traffic['zone'] = lat_idx * nb_horizontal + lon_idx\n",
    "\n",
    "assert traffic['zone'].max() <= nb_horizontal * nb_vertical\n",
    "\n",
    "# Drop lat/lon as we will now use the zone\n",
    "traffic.drop(['lat', 'lon'], axis=1)\n",
    "\n",
    "\n",
    "print(traffic.head())\n",
    "print(air_quality.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ecc1a",
   "metadata": {},
   "source": [
    "# Merging all the data\n",
    "We are then going to merge both of the dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615fd52",
   "metadata": {},
   "source": [
    "## Modification of the `traffic` dataframe\n",
    "Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour. Then we will melt the dataframe so that [hour, date] is a primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88fdc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone        date  hour  0  2  3  4  10  11  12  13  ...  47  51  52  53  54  \\\n",
      "0     2024-02-01    10  0  2  2  1   1   1   2   2  ...   1   2   0   1   0   \n",
      "1     2024-02-01    11  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "2     2024-02-01    12  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "3     2024-02-01    13  0  2  2  1   2   2   2   2  ...   1   2   0   1   0   \n",
      "4     2024-02-01    14  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "\n",
      "zone  55  60  61  62  63  \n",
      "0      0   0   0   2   2  \n",
      "1      0   0   0   2   2  \n",
      "2      0   0   0   2   2  \n",
      "3      0   0   0   2   2  \n",
      "4      0   0   0   2   3  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Covnert the 'data' (date) column of traffic dataframe\n",
    "traffic[\"datetime\"] = pd.to_datetime(traffic[\"data\"], format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour \n",
    "traffic[\"datetime_hour\"] = traffic[\"datetime\"].dt.round(\"h\")\n",
    "traffic[\"date\"] = traffic[\"datetime_hour\"].dt.date\n",
    "traffic[\"hour\"] = traffic[\"datetime_hour\"].dt.hour\n",
    "\n",
    "\n",
    "# Melting\n",
    "traffic = traffic.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['zone'],\n",
    "    values='estatActual',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "print(traffic.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bf4c1",
   "metadata": {},
   "source": [
    "## Modification of the `air_quality` dataframe\n",
    "The hours are currently in the columns, we are going to melt the dataframe to have a long one with dates in the rows.\n",
    "\n",
    "We also want [hour,date] to be a primary key so that merging both dataframes does not explodes RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b4a5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  hour  pollutant_10_station_4  pollutant_10_station_43  \\\n",
      "0  2024-02-01     1                    23.0                     30.0   \n",
      "1  2024-02-01     2                    19.0                     45.0   \n",
      "2  2024-02-01     3                    19.0                     45.0   \n",
      "3  2024-02-01     4                    19.0                     41.0   \n",
      "4  2024-02-01     5                    19.0                     36.0   \n",
      "\n",
      "   pollutant_10_station_44  pollutant_10_station_54  pollutant_10_station_57  \\\n",
      "0                     40.0                      NaN                     20.0   \n",
      "1                     35.0                      NaN                     20.0   \n",
      "2                     34.0                      NaN                     19.0   \n",
      "3                     32.0                      NaN                     24.0   \n",
      "4                     29.0                      NaN                     24.0   \n",
      "\n",
      "   pollutant_10_station_58  pollutant_110_station_57  \n",
      "0                     13.0                      19.6  \n",
      "1                     12.0                      20.2  \n",
      "2                     10.0                      18.4  \n",
      "3                     15.0                      23.5  \n",
      "4                     12.0                      23.7  \n"
     ]
    }
   ],
   "source": [
    "# Get the hours columns name\n",
    "hour_cols = [col for col in air_quality.columns if col.startswith(\"H\") and len(col) == 3]\n",
    "\n",
    "# Then use pandas melt method to reshape the DF\n",
    "\n",
    "aq_melted = air_quality.melt(\n",
    "    # Columns left untouched\n",
    "    id_vars=[\"CODI_PROVINCIA\", \"PROVINCIA\", \"CODI_MUNICIPI\", \"MUNICIPI\", \"ESTACIO\",\n",
    "             \"CODI_CONTAMINANT\", \"ANY\", \"MES\", \"DIA\"],\n",
    "\n",
    "    #Columns to unpivot\n",
    "    value_vars=hour_cols,\n",
    "\n",
    "    #Name of the new column that will store the unpivoted columns names (H01 etc.)\n",
    "    var_name=\"hour_str\",\n",
    "\n",
    "    #Name of the actual value column\n",
    "    value_name=\"pollution_value\"\n",
    ")\n",
    "    \n",
    "# Clean the hour (H01->1)\n",
    "aq_melted[\"hour\"] = aq_melted[\"hour_str\"].str.extract(r\"H(\\d{2})\").astype(\"int32\")\n",
    "\n",
    "# Create the actual date column (and translate column names)\n",
    "aq_melted[\"date\"] = pd.to_datetime(\n",
    "    aq_melted[[\"ANY\", \"MES\", \"DIA\"]].rename(columns={\"ANY\": \"year\", \"MES\": \"month\", \"DIA\": \"day\"})\n",
    ").dt.date\n",
    "\n",
    "\n",
    "\n",
    "# Pivot (CODI_CONTAMINANT,ESTACIO) so that [hour, date] is primary key\n",
    "aq_pivoted = aq_melted.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['CODI_CONTAMINANT', 'ESTACIO'],\n",
    "    values='pollution_value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Simple flattening (for more meaningful column names)\n",
    "aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'pollutant_{col[0]}_station_{col[1]}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "]\n",
    "\n",
    "print(aq_pivoted.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c08b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe `traffic`:\n",
      "zone        date  hour  0  2  3  4  10  11  12  13  ...  47  51  52  53  54  \\\n",
      "0     2024-02-01    10  0  2  2  1   1   1   2   2  ...   1   2   0   1   0   \n",
      "1     2024-02-01    11  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "2     2024-02-01    12  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "3     2024-02-01    13  0  2  2  1   2   2   2   2  ...   1   2   0   1   0   \n",
      "4     2024-02-01    14  0  2  2  1   1   2   2   2  ...   1   2   0   2   0   \n",
      "\n",
      "zone  55  60  61  62  63  \n",
      "0      0   0   0   2   2  \n",
      "1      0   0   0   2   2  \n",
      "2      0   0   0   2   2  \n",
      "3      0   0   0   2   2  \n",
      "4      0   0   0   2   3  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "Dataframe aq: \n",
      "         date  hour  pollutant_10_station_4  pollutant_10_station_43  \\\n",
      "0  2024-02-01     1                    23.0                     30.0   \n",
      "1  2024-02-01     2                    19.0                     45.0   \n",
      "2  2024-02-01     3                    19.0                     45.0   \n",
      "3  2024-02-01     4                    19.0                     41.0   \n",
      "4  2024-02-01     5                    19.0                     36.0   \n",
      "\n",
      "   pollutant_10_station_44  pollutant_10_station_54  pollutant_10_station_57  \\\n",
      "0                     40.0                      NaN                     20.0   \n",
      "1                     35.0                      NaN                     20.0   \n",
      "2                     34.0                      NaN                     19.0   \n",
      "3                     32.0                      NaN                     24.0   \n",
      "4                     29.0                      NaN                     24.0   \n",
      "\n",
      "   pollutant_10_station_58  pollutant_110_station_57  \n",
      "0                     13.0                      19.6  \n",
      "1                     12.0                      20.2  \n",
      "2                     10.0                      18.4  \n",
      "3                     15.0                      23.5  \n",
      "4                     12.0                      23.7  \n",
      "Final DF\n",
      "         date  hour  0  2  3  4  10  11  12  13  ...  61  62  63  \\\n",
      "0  2024-02-01    10  0  2  2  1   1   1   2   2  ...   0   2   2   \n",
      "1  2024-02-01    11  0  2  2  1   1   2   2   2  ...   0   2   2   \n",
      "2  2024-02-01    12  0  2  2  1   1   2   2   2  ...   0   2   2   \n",
      "3  2024-02-01    13  0  2  2  1   2   2   2   2  ...   0   2   2   \n",
      "4  2024-02-01    14  0  2  2  1   1   2   2   2  ...   0   2   3   \n",
      "\n",
      "   pollutant_10_station_4  pollutant_10_station_43  pollutant_10_station_44  \\\n",
      "0                    51.0                     69.0                     57.0   \n",
      "1                    51.0                     58.0                     57.0   \n",
      "2                    51.0                     51.0                     48.0   \n",
      "3                    49.0                     46.0                     41.0   \n",
      "4                    44.0                     46.0                     35.0   \n",
      "\n",
      "   pollutant_10_station_54  pollutant_10_station_57  pollutant_10_station_58  \\\n",
      "0                      NaN                     45.0                      NaN   \n",
      "1                      NaN                     25.0                      NaN   \n",
      "2                      NaN                     34.0                      NaN   \n",
      "3                      NaN                     24.0                     16.0   \n",
      "4                      NaN                     31.0                     17.0   \n",
      "\n",
      "   pollutant_110_station_57  \n",
      "0                      44.8  \n",
      "1                      25.0  \n",
      "2                      33.6  \n",
      "3                      23.7  \n",
      "4                      30.6  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "taux de valeurs pourries\n",
      "0.008731274108184075\n"
     ]
    }
   ],
   "source": [
    "# Now do the actual merging based on time\n",
    "print(\"Dataframe `traffic`:\")\n",
    "print(traffic.head())\n",
    "print(\"Dataframe aq: \")\n",
    "print(aq_pivoted.head())\n",
    "merged = pd.merge(\n",
    "    traffic,\n",
    "    aq_pivoted,\n",
    "    how=\"left\",\n",
    "    left_on=[\"date\", \"hour\"],\n",
    "    right_on=[\"date\", \"hour\"]\n",
    ")\n",
    "\n",
    "# is_primary_key = not traffic.duplicated(subset=['date', 'hour']).any()\n",
    "# print(f\"Is [date, hour] a primary key? {is_primary_key}\")\n",
    "\n",
    "print(\"Final DF\")\n",
    "print(merged.head())\n",
    "\n",
    "print(\"taux de valeurs pourries\")\n",
    "print(merged.isna().sum().sum() / merged.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6c1a1",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- nettoyer air quality: enlever les colonnes de polluants qui ne sont pas dans la référence(https://opendata-ajuntament.barcelona.cat/data/dataset/6960936a-95ed-4cc4-a6ec-e089197ccd8b/resource/c122329d-d26d-469e-bf9e-8efa10e4c127/download/qualitat_aire_contaminants.csv)\n",
    "dans la référence, les polluants avec une * sont plus précis.\n",
    "\n",
    "\n",
    "- se concentrer sur un seul/2 polluant (PM10=10, PM2.5=9)\n",
    "\n",
    "- Gérer les données vides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
