{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a9c3a4",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ab3b8",
   "metadata": {},
   "source": [
    "# Downloading data (Traffic and Air quality). \n",
    "Automatically downloads from the following dates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_from = datetime(2024, 1, 1)  # Example: January 2023\n",
    "download_until = datetime(2024, 4, 30) # Or set a specific end date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff831cb",
   "metadata": {},
   "source": [
    "Then run the following to actually download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed67fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwonloading files...\n",
      "File already exists: data/TRAMS_2024_01_Gener.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_01_Gener.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_03_Marc.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_03_Marc.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_04_Abril.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_04_Abril.csv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Catalan month names\n",
    "catalan_months = {\n",
    "    1: \"Gener\",\n",
    "    2: \"Febrer\",\n",
    "    3: \"Marc\",\n",
    "    4: \"Abril\",\n",
    "    5: \"Maig\",\n",
    "    6: \"Juny\",\n",
    "    7: \"Juliol\",\n",
    "    8: \"Agost\",\n",
    "    9: \"Setembre\",\n",
    "    10: \"Octubre\",\n",
    "    11: \"Novembre\",\n",
    "    12: \"Desembre\",\n",
    "}\n",
    "\n",
    "# Trams transit relacio (relation between ids and locations)\n",
    "trams_relacio_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/1090983a-1c40-4609-8620-14ad49aae3ab/resource/1d6c814c-70ef-4147-aa16-a49ddb952f72/download/transit_relacio_trams.csv\"\n",
    "trams_relacio_path = \"./data/transit_relacio_trams.csv\"\n",
    "\n",
    "# Air quality stations info (including lat and long)\n",
    "# (The stations are unchanged since 2023 so downloading only the 2025 version is enough)\n",
    "air_stations_info_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/4dff88b1-151b-48db-91c2-45007cd5d07a/resource/d1aa40d7-66f9-451b-85f8-955b765fdc2f/download/2025_qualitat_aire_estacions.csv\"\n",
    "air_stations_info_path = \"./data/air_stations_info.csv\"\n",
    "\n",
    "def generate_urls(download_from, download_until):\n",
    "    urls = []\n",
    "    current_date = datetime(download_from.year, download_from.month, 1)\n",
    "    end_date = datetime(download_until.year, download_until.month, 1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        month_name = catalan_months[month]\n",
    "\n",
    "        # Generate URLs for both datasets\n",
    "        tram_url = f\"https://opendata-ajuntament.barcelona.cat/resources/auto/transit/{year}_{month:02d}_{month_name}_TRAMS_TRAMS.csv\"\n",
    "        aire_url = f\"https://opendata-ajuntament.barcelona.cat/resources/bcn/QualitatAire/{year}_{month:02d}_{month_name}_qualitat_aire_BCN.csv\"\n",
    "\n",
    "        tram_filename = f\"data/TRAMS_{year}_{month:02d}_{month_name}.csv\"\n",
    "        aire_filename = f\"data/QualitatAire_{year}_{month:02d}_{month_name}.csv\"\n",
    "\n",
    "        urls.append((tram_url, tram_filename))\n",
    "        urls.append((aire_url, aire_filename))\n",
    "\n",
    "        # Move to the next month\n",
    "        if current_date.month == 12:\n",
    "            current_date = datetime(current_date.year + 1, 1, 1)\n",
    "        else:\n",
    "            current_date = datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Create a \"data\" folder:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "urls = generate_urls(download_from, download_until)\n",
    "\n",
    "print(\"Dwonloading files...\")\n",
    "for url, filename in urls:\n",
    "    download_file(url, filename)\n",
    "\n",
    "\n",
    "# Then download the TRAMS relacio file\n",
    "if not os.path.exists(trams_relacio_path):\n",
    "    download_file(trams_relacio_url, trams_relacio_path)\n",
    "\n",
    "\n",
    "# And the air stations info \n",
    "if not os.path.exists(air_stations_info_path):\n",
    "    download_file(air_stations_info_url, air_stations_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aec697",
   "metadata": {},
   "source": [
    "# Data treatment and cleaning\n",
    "\n",
    "\n",
    "## Creating a merged dataset\n",
    "\n",
    "### Combining all the air quality files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e437d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "0               8  Barcelona             19  Barcelona        4   \n",
      "1               8  Barcelona             19  Barcelona        4   \n",
      "2               8  Barcelona             19  Barcelona        4   \n",
      "3               8  Barcelona             19  Barcelona        4   \n",
      "4               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "   CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H21  V21   H22  V22   H23  \\\n",
      "0                 7  2024    1    1   3.0  ...  15.0    V  44.0    V  20.0   \n",
      "1                 7  2024    1    2  14.0  ...   2.0    V   3.0    V   6.0   \n",
      "2                 7  2024    1    3   2.0  ...   2.0    V   1.0    V   2.0   \n",
      "3                 7  2024    1    4  20.0  ...   2.0    V  29.0    V  57.0   \n",
      "4                 7  2024    1    5  29.0  ...   1.0    V   2.0    V   1.0   \n",
      "\n",
      "   V23   H24  V24  Latitud  Longitud  \n",
      "0    V  12.0    V  41.4039    2.2045  \n",
      "1    V   2.0    V  41.4039    2.2045  \n",
      "2    V   3.0    V  41.4039    2.2045  \n",
      "3    V  47.0    V  41.4039    2.2045  \n",
      "4    V   1.0    V  41.4039    2.2045  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "air_quality_files = [filename for url, filename in urls if \"QualitatAire\" in filename]\n",
    "#print(air_quality_files)\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "air_quality_dfs = [pd.read_csv(file) for file in air_quality_files]\n",
    "# Combine them into a single dataframe\n",
    "air_quality_combined = pd.concat(air_quality_dfs, ignore_index=True)\n",
    "#print(air_quality_combined.head())\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "air_station_info = pd.read_csv(air_stations_info_path)\n",
    "# Only keep location data for each station\n",
    "air_station_locations = air_station_info[['Estacio', 'Latitud', 'Longitud']].drop_duplicates(subset=['Estacio'])\n",
    "#print(air_station_locations.head(n=20))\n",
    "\n",
    "## Now merge the air_quality measurements and the location\n",
    "\n",
    "air_quality = air_quality_combined.merge(\n",
    "    right=air_station_locations,\n",
    "    left_on='ESTACIO',\n",
    "    right_on='Estacio',\n",
    "    how='left'\n",
    ").drop(columns=['Estacio'])\n",
    "\n",
    "print(air_quality.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d460",
   "metadata": {},
   "source": [
    "### Combining traffic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9908b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual  estatPrevist\n",
      "0       1  20240101000552            0             0\n",
      "1       2  20240101000552            0             0\n",
      "2       3  20240101000552            0             0\n",
      "3       4  20240101000552            0             0\n",
      "4       5  20240101000552            0             0\n",
      "RESULT FINA\n",
      "   idTram            data  estatActual       lat        lon\n",
      "0       1  20240101000552            0  2.106769  41.382911\n",
      "1       2  20240101000552            0  2.106769  41.383167\n",
      "2       3  20240101000552            0  2.117372  41.385579\n",
      "3       4  20240101000552            0  2.117281  41.385824\n",
      "4       5  20240101000552            0  2.125109  41.387561\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "traffic_files = [filename for url, filename in urls if \"TRAMS\" in filename]\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "traffic_dfs = [pd.read_csv(file) for file in traffic_files]\n",
    "#print(traffic_dfs[0].head())\n",
    "\n",
    "# Combine them into a single dataframe\n",
    "traffic_combined = pd.concat(traffic_dfs, ignore_index=True)\n",
    "print(traffic_combined.head())\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "trams_info = pd.read_csv(trams_relacio_path)\n",
    "# Only keep location data for each station\n",
    "trams_locations = trams_info[['Tram', 'Coordenades']]\n",
    "\n",
    "# Only keep one location point per section (tram=section)\n",
    "def mean_coordinate(row):\n",
    "    # print(\"entering function mean\")\n",
    "    # print(row)\n",
    "    coord_text=row['Coordenades']\n",
    "    numbers=coord_text.split(',')\n",
    "    assert len(numbers) % 2 == 0\n",
    "    lats = [float(x) for x in numbers[::2]]\n",
    "    lons = [float(x) for x in numbers[1::2]]\n",
    "    # print(\"infos\")\n",
    "    # print(lats)\n",
    "    # print(type(lats[0]))\n",
    "    lat = np.mean(lats)\n",
    "    lon = np.mean(lons)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "trams_locations [['lat' , 'lon']] = trams_locations.apply(mean_coordinate, axis=1)\n",
    "#print(trams_locations.head())\n",
    "\n",
    "\n",
    "## Now merge the traffic measurements and the location\n",
    "traffic = traffic_combined.merge(\n",
    "    right=trams_locations,\n",
    "    left_on='idTram',\n",
    "    right_on='Tram',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "traffic = traffic.drop(['Coordenades', 'Tram', 'estatPrevist'], axis=1)\n",
    "\n",
    "print(\"RESULT FINA\")\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364615f",
   "metadata": {},
   "source": [
    "## Dataset issue\n",
    "\n",
    "For some reason, the city does not provide information for the 'trams' with id greater than 527. In the `traffic` dataframe, there are some records with idTram between 535 and 539. They don't have a latitude/longitude. Let us drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b751bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = traffic.dropna(subset=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6d93",
   "metadata": {},
   "source": [
    "# Defining a traffic grid\n",
    "\n",
    "Now, `traffic` contains the state (from 0-no car to 6-congestioned) of plenty of coordinates in Barcelona. But the density is not homogeneous, hence i will cut the city in a grid and compute an average value for traffic congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb04e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual       lat        lon  zone\n",
      "0       1  20240101000552            0  2.106769  41.382911     2\n",
      "1       2  20240101000552            0  2.106769  41.383167     2\n",
      "2       3  20240101000552            0  2.117372  41.385579     3\n",
      "3       4  20240101000552            0  2.117281  41.385824     3\n",
      "4       5  20240101000552            0  2.125109  41.387561    11\n",
      "   CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "0               8  Barcelona             19  Barcelona        4   \n",
      "1               8  Barcelona             19  Barcelona        4   \n",
      "2               8  Barcelona             19  Barcelona        4   \n",
      "3               8  Barcelona             19  Barcelona        4   \n",
      "4               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "   CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H21  V21   H22  V22   H23  \\\n",
      "0                 7  2024    1    1   3.0  ...  15.0    V  44.0    V  20.0   \n",
      "1                 7  2024    1    2  14.0  ...   2.0    V   3.0    V   6.0   \n",
      "2                 7  2024    1    3   2.0  ...   2.0    V   1.0    V   2.0   \n",
      "3                 7  2024    1    4  20.0  ...   2.0    V  29.0    V  57.0   \n",
      "4                 7  2024    1    5  29.0  ...   1.0    V   2.0    V   1.0   \n",
      "\n",
      "   V23   H24  V24  Latitud  Longitud  \n",
      "0    V  12.0    V  41.4039    2.2045  \n",
      "1    V   2.0    V  41.4039    2.2045  \n",
      "2    V   3.0    V  41.4039    2.2045  \n",
      "3    V  47.0    V  41.4039    2.2045  \n",
      "4    V   1.0    V  41.4039    2.2045  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "# Getting the boundaries of the traffic information\n",
    "minlat = traffic['lat'].min()\n",
    "maxlat = traffic['lat'].max()\n",
    "minlon = traffic['lon'].min()\n",
    "maxlon = traffic['lon'].max()\n",
    "\n",
    "#TODO add prints of the distance of the box here\n",
    "\n",
    "\n",
    "nb_horizontal = 8\n",
    "nb_vertical = 8\n",
    "\n",
    "vertical_step = (maxlat - minlat) / nb_vertical\n",
    "horizontal_step = (maxlon - minlon) / nb_horizontal\n",
    "\n",
    "\n",
    "# Create new datafram from traffic\n",
    "\n",
    "\"\"\" def getZone(row):\n",
    "    lat = (row['lat'] - minlat) // vertical_step\n",
    "    lon = (row['lon'] - minlon) // horizontal_step\n",
    "    zone_number = (lat * nb_horizontal) + lon\n",
    "\n",
    "\n",
    "traffic['zone'] = traffic.apply(getZone, axis=1) \"\"\"\n",
    "# BETTER WAY (vectorized)\n",
    "# Create nb_vertical * nb_horizontal zones and assign every row a zone:\n",
    "lat_idx = np.clip(np.floor((traffic['lat'] - minlat) / vertical_step).astype(int), 0, nb_vertical - 1)\n",
    "lon_idx = np.clip(np.floor((traffic['lon'] - minlon) / horizontal_step).astype(int), 0, nb_horizontal - 1)\n",
    "traffic['zone'] = lat_idx * nb_horizontal + lon_idx\n",
    "\n",
    "assert traffic['zone'].max() <= nb_horizontal * nb_vertical\n",
    "\n",
    "# Drop lat/lon as we will now use the zone\n",
    "traffic.drop(['lat', 'lon'], axis=1)\n",
    "\n",
    "\n",
    "print(traffic.head())\n",
    "print(air_quality.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec5ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
