{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a9c3a4",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ec0af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ab3b8",
   "metadata": {},
   "source": [
    "# Downloading data (Traffic and Air quality). \n",
    "Automatically downloads from the following dates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bd2bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_from = datetime(2024, 1, 1)  # Example: January 2023\n",
    "download_until = datetime(2024, 10, 28) # Or set a specific end date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff831cb",
   "metadata": {},
   "source": [
    "Then run the following to actually download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ed67fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwonloading files...\n",
      "File already exists: data/TRAMS_2024_01_Gener.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_01_Gener.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_03_Marc.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_03_Marc.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_04_Abril.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_04_Abril.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_05_Maig.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_05_Maig.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_06_Juny.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_06_Juny.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_07_Juliol.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_07_Juliol.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_08_Agost.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_08_Agost.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_09_Setembre.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_09_Setembre.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_10_Octubre.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_10_Octubre.csv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Catalan month names\n",
    "catalan_months = {\n",
    "    1: \"Gener\",\n",
    "    2: \"Febrer\",\n",
    "    3: \"Marc\",\n",
    "    4: \"Abril\",\n",
    "    5: \"Maig\",\n",
    "    6: \"Juny\",\n",
    "    7: \"Juliol\",\n",
    "    8: \"Agost\",\n",
    "    9: \"Setembre\",\n",
    "    10: \"Octubre\",\n",
    "    11: \"Novembre\",\n",
    "    12: \"Desembre\",\n",
    "}\n",
    "\n",
    "# Trams transit relacio (relation between ids and locations)\n",
    "trams_relacio_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/1090983a-1c40-4609-8620-14ad49aae3ab/resource/1d6c814c-70ef-4147-aa16-a49ddb952f72/download/transit_relacio_trams.csv\"\n",
    "trams_relacio_path = \"./data/transit_relacio_trams.csv\"\n",
    "\n",
    "# Air quality stations info (including lat and long)\n",
    "# (The stations are unchanged since 2023 so downloading only the 2025 version is enough)\n",
    "air_stations_info_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/4dff88b1-151b-48db-91c2-45007cd5d07a/resource/d1aa40d7-66f9-451b-85f8-955b765fdc2f/download/2025_qualitat_aire_estacions.csv\"\n",
    "air_stations_info_path = \"./data/air_stations_info.csv\"\n",
    "\n",
    "def generate_urls(download_from, download_until):\n",
    "    urls = []\n",
    "    current_date = datetime(download_from.year, download_from.month, 1)\n",
    "    end_date = datetime(download_until.year, download_until.month, 1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        month_name = catalan_months[month]\n",
    "\n",
    "        # Generate URLs for both datasets\n",
    "        tram_url = f\"https://opendata-ajuntament.barcelona.cat/resources/auto/transit/{year}_{month:02d}_{month_name}_TRAMS_TRAMS.csv\"\n",
    "        aire_url = f\"https://opendata-ajuntament.barcelona.cat/resources/bcn/QualitatAire/{year}_{month:02d}_{month_name}_qualitat_aire_BCN.csv\"\n",
    "\n",
    "        tram_filename = f\"data/TRAMS_{year}_{month:02d}_{month_name}.csv\"\n",
    "        aire_filename = f\"data/QualitatAire_{year}_{month:02d}_{month_name}.csv\"\n",
    "\n",
    "        urls.append((tram_url, tram_filename))\n",
    "        urls.append((aire_url, aire_filename))\n",
    "\n",
    "        # Move to the next month\n",
    "        if current_date.month == 12:\n",
    "            current_date = datetime(current_date.year + 1, 1, 1)\n",
    "        else:\n",
    "            current_date = datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Create a \"data\" folder:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "urls = generate_urls(download_from, download_until)\n",
    "\n",
    "print(\"Dwonloading files...\")\n",
    "for url, filename in urls:\n",
    "    download_file(url, filename)\n",
    "\n",
    "\n",
    "# Then download the TRAMS relacio file\n",
    "if not os.path.exists(trams_relacio_path):\n",
    "    download_file(trams_relacio_url, trams_relacio_path)\n",
    "\n",
    "\n",
    "# And the air stations info \n",
    "if not os.path.exists(air_stations_info_path):\n",
    "    download_file(air_stations_info_url, air_stations_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aec697",
   "metadata": {},
   "source": [
    "# Data treatment and cleaning\n",
    "\n",
    "\n",
    "## Creating a merged dataset\n",
    "\n",
    "### Combining all the air quality files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5e437d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "62               8  Barcelona             19  Barcelona        4   \n",
      "63               8  Barcelona             19  Barcelona        4   \n",
      "64               8  Barcelona             19  Barcelona        4   \n",
      "65               8  Barcelona             19  Barcelona        4   \n",
      "66               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "    CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H20  V20   H21  V21   H22  \\\n",
      "62                10  2024    1    1  34.0  ...  13.0    V  14.0    V  25.0   \n",
      "63                10  2024    1    2  19.0  ...  36.0    V  37.0    V  29.0   \n",
      "64                10  2024    1    3  30.0  ...  29.0    V  19.0    V  15.0   \n",
      "65                10  2024    1    4  25.0  ...  30.0    V  28.0    V  27.0   \n",
      "66                10  2024    1    5  26.0  ...  13.0    V   7.0    V   5.0   \n",
      "\n",
      "    V22   H23  V23   H24  V24  \n",
      "62    V  21.0    V  19.0    V  \n",
      "63    V  29.0    V  24.0    V  \n",
      "64    V  19.0    V  19.0    V  \n",
      "65    V  33.0    V  35.0    V  \n",
      "66    V   5.0    V   5.0    V  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" ## Now merge the air_quality measurements and the location\\n\\nair_quality = air_quality_combined.merge(\\n    right=air_station_locations,\\n    left_on='ESTACIO',\\n    right_on='Estacio',\\n    how='left'\\n).drop(columns=['Estacio'])\\n\\nprint(air_quality.head()) \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "air_quality_files = [filename for url, filename in urls if \"QualitatAire\" in filename]\n",
    "#print(air_quality_files)\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "air_quality_dfs = [pd.read_csv(file) for file in air_quality_files]\n",
    "# Combine them into a single dataframe\n",
    "air_quality = pd.concat(air_quality_dfs, ignore_index=True)\n",
    "#print(air_quality_combined.head())\n",
    "\n",
    "\n",
    "\n",
    "# Only keep PM10 DATA\n",
    "air_quality = air_quality[air_quality[\"CODI_CONTAMINANT\"].isin([10, 110])]\n",
    "print(air_quality.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "air_station_info = pd.read_csv(air_stations_info_path)\n",
    "# Only keep location data for each station\n",
    "air_station_locations = air_station_info[['Estacio', 'Latitud', 'Longitud']].drop_duplicates(subset=['Estacio'])\n",
    "#print(air_station_locations.head(n=20))\n",
    "\n",
    "\"\"\" ## Now merge the air_quality measurements and the location\n",
    "\n",
    "air_quality = air_quality_combined.merge(\n",
    "    right=air_station_locations,\n",
    "    left_on='ESTACIO',\n",
    "    right_on='Estacio',\n",
    "    how='left'\n",
    ").drop(columns=['Estacio'])\n",
    "\n",
    "print(air_quality.head()) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d460",
   "metadata": {},
   "source": [
    "### Combining traffic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9908b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual  estatPrevist\n",
      "0       1  20240101000552            0             0\n",
      "1       2  20240101000552            0             0\n",
      "2       3  20240101000552            0             0\n",
      "3       4  20240101000552            0             0\n",
      "4       5  20240101000552            0             0\n",
      "RESULT FINA\n",
      "   idTram            data  estatActual       lat        lon\n",
      "0       1  20240101000552            0  2.106769  41.382911\n",
      "1       2  20240101000552            0  2.106769  41.383167\n",
      "2       3  20240101000552            0  2.117372  41.385579\n",
      "3       4  20240101000552            0  2.117281  41.385824\n",
      "4       5  20240101000552            0  2.125109  41.387561\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "traffic_files = [filename for url, filename in urls if \"TRAMS\" in filename]\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "traffic_dfs = [pd.read_csv(file) for file in traffic_files]\n",
    "#print(traffic_dfs[0].head())\n",
    "\n",
    "# Combine them into a single dataframe\n",
    "traffic_combined = pd.concat(traffic_dfs, ignore_index=True)\n",
    "print(traffic_combined.head())\n",
    "\n",
    "\n",
    "# Drop the rows with no data (estatActual=0)\n",
    "traffic_combined = traffic_combined.query(\"estatActual != 0\")\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "trams_info = pd.read_csv(trams_relacio_path)\n",
    "# Only keep location data for each station\n",
    "trams_locations = trams_info[['Tram', 'Coordenades']]\n",
    "\n",
    "# Only keep one location point per section (tram=section)\n",
    "def mean_coordinate(row):\n",
    "    # print(\"entering function mean\")\n",
    "    # print(row)\n",
    "    coord_text=row['Coordenades']\n",
    "    numbers=coord_text.split(',')\n",
    "    assert len(numbers) % 2 == 0\n",
    "    lats = [float(x) for x in numbers[::2]]\n",
    "    lons = [float(x) for x in numbers[1::2]]\n",
    "    # print(\"infos\")\n",
    "    # print(lats)\n",
    "    # print(type(lats[0]))\n",
    "    lat = np.mean(lats)\n",
    "    lon = np.mean(lons)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "trams_locations [['lat' , 'lon']] = trams_locations.apply(mean_coordinate, axis=1)\n",
    "#print(trams_locations.head())\n",
    "\n",
    "\n",
    "## Now merge the traffic measurements and the location\n",
    "traffic = traffic_combined.merge(\n",
    "    right=trams_locations,\n",
    "    left_on='idTram',\n",
    "    right_on='Tram',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "traffic = traffic.drop(['Coordenades', 'Tram', 'estatPrevist'], axis=1)\n",
    "\n",
    "print(\"RESULT FINA\")\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364615f",
   "metadata": {},
   "source": [
    "## Dataset issue\n",
    "\n",
    "For some reason, the city does not provide information for the 'trams' with id greater than 527. In the `traffic` dataframe, there are some records with idTram between 535 and 539. They don't have a latitude/longitude. Let us drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b751bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = traffic.dropna(subset=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6d93",
   "metadata": {},
   "source": [
    "# Defining a traffic grid\n",
    "\n",
    "Now, `traffic` contains the state (from 0-no car to 6-congestioned) of plenty of coordinates in Barcelona. But the density is not homogeneous, hence i will cut the city in a grid and compute an average value for traffic congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3eb04e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual       lat        lon  zone\n",
      "0       1  20240101000552            0  2.106769  41.382911     2\n",
      "1       2  20240101000552            0  2.106769  41.383167     2\n",
      "2       3  20240101000552            0  2.117372  41.385579     2\n",
      "3       4  20240101000552            0  2.117281  41.385824     2\n",
      "4       5  20240101000552            0  2.125109  41.387561     8\n"
     ]
    }
   ],
   "source": [
    "# Getting the boundaries of the traffic information\n",
    "minlat = traffic['lat'].min()\n",
    "maxlat = traffic['lat'].max()\n",
    "minlon = traffic['lon'].min()\n",
    "maxlon = traffic['lon'].max()\n",
    "\n",
    "#TODO add prints of the distance of the box here\n",
    "\n",
    "\n",
    "nb_horizontal = 6\n",
    "nb_vertical = 6\n",
    "\n",
    "vertical_step = (maxlat - minlat) / nb_vertical\n",
    "horizontal_step = (maxlon - minlon) / nb_horizontal\n",
    "\n",
    "\n",
    "# Create new datafram from traffic\n",
    "\n",
    "\"\"\" def getZone(row):\n",
    "    lat = (row['lat'] - minlat) // vertical_step\n",
    "    lon = (row['lon'] - minlon) // horizontal_step\n",
    "    zone_number = (lat * nb_horizontal) + lon\n",
    "\n",
    "\n",
    "traffic['zone'] = traffic.apply(getZone, axis=1) \"\"\"\n",
    "# BETTER WAY (vectorized)\n",
    "# Create nb_vertical * nb_horizontal zones and assign every row a zone:\n",
    "lat_idx = np.clip(np.floor((traffic['lat'] - minlat) / vertical_step).astype(int), 0, nb_vertical - 1)\n",
    "lon_idx = np.clip(np.floor((traffic['lon'] - minlon) / horizontal_step).astype(int), 0, nb_horizontal - 1)\n",
    "traffic['zone'] = lat_idx * nb_horizontal + lon_idx\n",
    "\n",
    "assert traffic['zone'].max() <= nb_horizontal * nb_vertical\n",
    "\n",
    "# Drop lat/lon as we will now use the zone\n",
    "traffic.drop(['lat', 'lon'], axis=1)\n",
    "\n",
    "\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ecc1a",
   "metadata": {},
   "source": [
    "# Merging all the data\n",
    "We are then going to merge both of the dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615fd52",
   "metadata": {},
   "source": [
    "## Modification of the `traffic` dataframe\n",
    "Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour. Then we will melt the dataframe so that [hour, date] is a primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "88fdc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone        date  hour  traffic_z0  traffic_z1  traffic_z2  traffic_z3  \\\n",
      "0     2024-01-01     0         0.0         0.0         0.0         0.0   \n",
      "1     2024-01-01     1         0.0         0.0         0.0         1.0   \n",
      "2     2024-01-01     2         0.0         0.0         0.0         1.0   \n",
      "3     2024-01-01     3         0.0         0.0         0.0         1.0   \n",
      "4     2024-01-01     4         0.0         0.0         0.0         0.0   \n",
      "\n",
      "zone  traffic_z6  traffic_z7  traffic_z8  traffic_z9  ...  traffic_z22  \\\n",
      "0            0.5         0.0         0.0         0.0  ...          0.0   \n",
      "1            0.5         0.0         1.0         1.0  ...          1.0   \n",
      "2            0.5         0.0         1.0         1.0  ...          1.0   \n",
      "3            0.5         0.0         1.0         1.0  ...          0.0   \n",
      "4            0.5         0.0         0.0         1.0  ...          0.0   \n",
      "\n",
      "zone  traffic_z23  traffic_z26  traffic_z27  traffic_z28  traffic_z29  \\\n",
      "0             0.0          0.0          0.0          0.0          0.0   \n",
      "1             1.0          1.0          0.0          0.0          1.0   \n",
      "2             1.0          1.0          0.0          0.0          1.0   \n",
      "3             1.0          1.0          0.0          0.0          1.0   \n",
      "4             0.0          1.0          0.0          0.0          0.0   \n",
      "\n",
      "zone  traffic_z32  traffic_z33  traffic_z34  traffic_z35  \n",
      "0             1.0          0.0          0.0          1.0  \n",
      "1             2.0          0.0          0.0          1.5  \n",
      "2             2.0          0.0          0.0          1.5  \n",
      "3             1.0          0.0          0.0          1.0  \n",
      "4             1.0          0.0          0.0          1.0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Covnert the 'data' (date) column of traffic dataframe\n",
    "traffic[\"datetime\"] = pd.to_datetime(traffic[\"data\"], format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour \n",
    "traffic[\"datetime_hour\"] = traffic[\"datetime\"].dt.round(\"h\")\n",
    "traffic[\"date\"] = traffic[\"datetime_hour\"].dt.date\n",
    "traffic[\"hour\"] = traffic[\"datetime_hour\"].dt.hour\n",
    "\n",
    "\n",
    "# Pivoting\n",
    "traffic = traffic.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['zone'],\n",
    "    values='estatActual',\n",
    "    aggfunc='median'\n",
    ").add_prefix('traffic_z').reset_index()\n",
    "print(traffic.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bf4c1",
   "metadata": {},
   "source": [
    "## Modification of the `air_quality` dataframe\n",
    "The hours are currently in the columns, we are going to melt the dataframe to have a long one with dates in the rows.\n",
    "\n",
    "We also want [hour,date] to be a primary key so that merging both dataframes does not explodes RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3b4a5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  hour  station_4  station_43  station_44  station_54  \\\n",
      "0    2024-01-01     1       34.0        43.0        33.0        23.0   \n",
      "1    2024-01-01     2       34.0        36.0        29.0        26.0   \n",
      "2    2024-01-01     3       34.0        30.0        32.0        18.0   \n",
      "3    2024-01-01     4       27.0        34.0        26.0        14.0   \n",
      "4    2024-01-01     5       24.0        35.0        28.0        18.0   \n",
      "..          ...   ...        ...         ...         ...         ...   \n",
      "195  2024-01-09     4       17.0        11.0         6.0         8.0   \n",
      "196  2024-01-09     5       13.0        10.0         7.0         6.0   \n",
      "197  2024-01-09     6       11.0        10.0         7.0         5.0   \n",
      "198  2024-01-09     7       13.0        12.0        12.0        10.0   \n",
      "199  2024-01-09     8       33.0        21.0        24.0        14.0   \n",
      "\n",
      "     station_57  station_58  \n",
      "0          16.0        14.0  \n",
      "1          25.0        18.0  \n",
      "2          21.0        14.0  \n",
      "3          16.0        15.0  \n",
      "4          17.0        13.0  \n",
      "..          ...         ...  \n",
      "195         5.0         8.0  \n",
      "196         5.0        10.0  \n",
      "197         6.0        12.0  \n",
      "198         8.0        11.0  \n",
      "199        11.0        15.0  \n",
      "\n",
      "[200 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the hours columns name\n",
    "hour_cols = [col for col in air_quality.columns if col.startswith(\"H\") and len(col) == 3]\n",
    "\n",
    "# Then use pandas melt method to reshape the DF\n",
    "\n",
    "aq_melted = air_quality.melt(\n",
    "    # Columns left untouched\n",
    "    id_vars=[\"CODI_PROVINCIA\", \"PROVINCIA\", \"CODI_MUNICIPI\", \"MUNICIPI\", \"ESTACIO\", \"ANY\", \"MES\", \"DIA\"], # IMPORTANT: i removed CODI_CONTAMINANT because i only kept PM10 temporarily\n",
    "\n",
    "    #Columns to unpivot\n",
    "    value_vars=hour_cols,\n",
    "\n",
    "    #Name of the new column that will store the unpivoted columns names (H01 etc.)\n",
    "    var_name=\"hour_str\",\n",
    "\n",
    "    #Name of the actual value column\n",
    "    value_name=\"pollution_value\"\n",
    ")\n",
    "    \n",
    "# Clean the hour (H01->1)\n",
    "aq_melted[\"hour\"] = aq_melted[\"hour_str\"].str.extract(r\"H(\\d{2})\").astype(\"int32\")\n",
    "\n",
    "\n",
    "\n",
    "# Create the actual date column (and translate column names)\n",
    "aq_melted[\"date\"] = pd.to_datetime(\n",
    "    aq_melted[[\"ANY\", \"MES\", \"DIA\"]].rename(columns={\"ANY\": \"year\", \"MES\": \"month\", \"DIA\": \"day\"})\n",
    ").dt.date\n",
    "\n",
    "#Shift the H24 columns to hour 0 of the next day:\n",
    "mask_24 = aq_melted[\"hour\"] == 24\n",
    "aq_melted.loc[mask_24, \"hour\"] = 0\n",
    "aq_melted.loc[mask_24, \"date\"] = pd.to_datetime(aq_melted.loc[mask_24, \"date\"]) + pd.Timedelta(days=1)\n",
    "aq_melted[\"date\"] = pd.to_datetime(aq_melted[\"date\"]).dt.date\n",
    "\n",
    "\n",
    "# Pivot (CODI_CONTAMINANT,ESTACIO) so that [hour, date] is primary key\n",
    "aq_pivoted = aq_melted.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['ESTACIO'], # IMPORTANT: here i also remove CODI_CONTAMINANT since there is only one.\n",
    "    values='pollution_value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Simple flattening (for more meaningful column names)\n",
    "\"\"\" aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'pollutant_{col[0]}_station_{col[1]}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "] \"\"\"\n",
    "aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'station_{col}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "]\n",
    "\n",
    "print(aq_pivoted.head(200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1c08b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe `traffic`:\n",
      "zone        date  hour  traffic_z0  traffic_z1  traffic_z2  traffic_z3  \\\n",
      "0     2024-01-01     0         0.0         0.0         0.0         0.0   \n",
      "1     2024-01-01     1         0.0         0.0         0.0         1.0   \n",
      "2     2024-01-01     2         0.0         0.0         0.0         1.0   \n",
      "3     2024-01-01     3         0.0         0.0         0.0         1.0   \n",
      "4     2024-01-01     4         0.0         0.0         0.0         0.0   \n",
      "\n",
      "zone  traffic_z6  traffic_z7  traffic_z8  traffic_z9  ...  traffic_z22  \\\n",
      "0            0.5         0.0         0.0         0.0  ...          0.0   \n",
      "1            0.5         0.0         1.0         1.0  ...          1.0   \n",
      "2            0.5         0.0         1.0         1.0  ...          1.0   \n",
      "3            0.5         0.0         1.0         1.0  ...          0.0   \n",
      "4            0.5         0.0         0.0         1.0  ...          0.0   \n",
      "\n",
      "zone  traffic_z23  traffic_z26  traffic_z27  traffic_z28  traffic_z29  \\\n",
      "0             0.0          0.0          0.0          0.0          0.0   \n",
      "1             1.0          1.0          0.0          0.0          1.0   \n",
      "2             1.0          1.0          0.0          0.0          1.0   \n",
      "3             1.0          1.0          0.0          0.0          1.0   \n",
      "4             0.0          1.0          0.0          0.0          0.0   \n",
      "\n",
      "zone  traffic_z32  traffic_z33  traffic_z34  traffic_z35  \n",
      "0             1.0          0.0          0.0          1.0  \n",
      "1             2.0          0.0          0.0          1.5  \n",
      "2             2.0          0.0          0.0          1.5  \n",
      "3             1.0          0.0          0.0          1.0  \n",
      "4             1.0          0.0          0.0          1.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Dataframe aq: \n",
      "         date  hour  station_4  station_43  station_44  station_54  \\\n",
      "0  2024-01-01     1       34.0        43.0        33.0        23.0   \n",
      "1  2024-01-01     2       34.0        36.0        29.0        26.0   \n",
      "2  2024-01-01     3       34.0        30.0        32.0        18.0   \n",
      "3  2024-01-01     4       27.0        34.0        26.0        14.0   \n",
      "4  2024-01-01     5       24.0        35.0        28.0        18.0   \n",
      "\n",
      "   station_57  station_58  \n",
      "0        16.0        14.0  \n",
      "1        25.0        18.0  \n",
      "2        21.0        14.0  \n",
      "3        16.0        15.0  \n",
      "4        17.0        13.0  \n",
      "Final DF\n",
      "          date  hour  traffic_z0  traffic_z1  traffic_z2  traffic_z3  \\\n",
      "0   2024-01-01     0         0.0         0.0         0.0         0.0   \n",
      "1   2024-01-01     1         0.0         0.0         0.0         1.0   \n",
      "2   2024-01-01     2         0.0         0.0         0.0         1.0   \n",
      "3   2024-01-01     3         0.0         0.0         0.0         1.0   \n",
      "4   2024-01-01     4         0.0         0.0         0.0         0.0   \n",
      "5   2024-01-01     5         0.0         0.0         0.0         0.0   \n",
      "6   2024-01-01     6         0.0         0.0         0.0         0.0   \n",
      "7   2024-01-01     7         0.0         0.0         0.0         0.0   \n",
      "8   2024-01-01     8         0.0         0.0         0.0         0.0   \n",
      "9   2024-01-01     9         0.0         0.0         0.0         0.0   \n",
      "10  2024-01-01    10         0.0         0.0         0.0         0.0   \n",
      "11  2024-01-01    11         0.0         0.0         1.0         1.0   \n",
      "12  2024-01-01    12         0.0         0.0         1.0         1.0   \n",
      "13  2024-01-01    13         0.0         0.0         2.0         2.0   \n",
      "14  2024-01-01    14         0.0         0.0         2.0         2.0   \n",
      "15  2024-01-01    15         0.0         0.0         1.0         1.0   \n",
      "16  2024-01-01    16         0.0         0.0         1.0         1.0   \n",
      "17  2024-01-01    17         0.0         0.0         1.0         1.0   \n",
      "18  2024-01-01    18         0.0         0.0         1.0         1.5   \n",
      "19  2024-01-01    19         0.0         0.0         1.0         2.0   \n",
      "20  2024-01-01    20         0.0         0.0         1.0         1.5   \n",
      "21  2024-01-01    21         0.0         0.0         1.0         1.0   \n",
      "22  2024-01-01    22         0.0         0.0         0.0         1.0   \n",
      "23  2024-01-01    23         0.0         0.0         0.0         1.0   \n",
      "24  2024-01-02     0         0.0         0.0         0.0         0.0   \n",
      "25  2024-01-02     1         0.0         0.0         0.0         0.0   \n",
      "26  2024-01-02     2         0.0         0.0         0.0         0.0   \n",
      "27  2024-01-02     3         0.0         0.0         0.0         0.0   \n",
      "28  2024-01-02     5         0.0         0.0         0.0         0.0   \n",
      "29  2024-01-02     6         0.0         0.0         0.0         0.5   \n",
      "\n",
      "    traffic_z6  traffic_z7  traffic_z8  traffic_z9  ...  traffic_z32  \\\n",
      "0          0.5         0.0         0.0         0.0  ...          1.0   \n",
      "1          0.5         0.0         1.0         1.0  ...          2.0   \n",
      "2          0.5         0.0         1.0         1.0  ...          2.0   \n",
      "3          0.5         0.0         1.0         1.0  ...          1.0   \n",
      "4          0.5         0.0         0.0         1.0  ...          1.0   \n",
      "5          0.0         0.0         0.0         0.0  ...          1.0   \n",
      "6          1.0         0.0         0.0         0.0  ...          1.0   \n",
      "7          1.5         0.0         0.0         0.0  ...          1.0   \n",
      "8          1.5         0.0         0.0         0.0  ...          1.0   \n",
      "9          1.5         0.0         0.0         0.0  ...          1.0   \n",
      "10         1.5         0.0         0.0         1.0  ...          1.0   \n",
      "11         1.5         0.0         0.0         1.0  ...          1.0   \n",
      "12         1.5         0.0         0.0         1.0  ...          2.0   \n",
      "13         1.5         1.0         1.0         1.0  ...          2.0   \n",
      "14         1.5         0.5         1.0         2.0  ...          2.0   \n",
      "15         1.5         0.0         1.0         1.0  ...          2.0   \n",
      "16         1.5         0.0         1.0         1.0  ...          2.0   \n",
      "17         2.0         0.0         1.0         1.0  ...          2.0   \n",
      "18         2.0         1.0         1.0         1.0  ...          2.0   \n",
      "19         2.0         1.0         1.0         1.0  ...          2.0   \n",
      "20         2.0         1.0         1.0         1.0  ...          2.0   \n",
      "21         1.5         1.0         1.0         1.0  ...          2.0   \n",
      "22         1.0         0.0         1.0         1.0  ...          1.0   \n",
      "23         0.5         0.0         0.0         1.0  ...          1.0   \n",
      "24         0.5         0.0         0.0         0.0  ...          1.0   \n",
      "25         0.5         0.0         0.0         0.0  ...          1.0   \n",
      "26         0.5         0.0         0.0         0.0  ...          1.0   \n",
      "27         0.5         0.0         0.0         0.0  ...          1.0   \n",
      "28         0.5         0.0         0.0         0.0  ...          1.0   \n",
      "29         1.0         0.0         0.0         0.0  ...          2.0   \n",
      "\n",
      "    traffic_z33  traffic_z34  traffic_z35  station_4  station_43  station_44  \\\n",
      "0           0.0          0.0          1.0        NaN         NaN         NaN   \n",
      "1           0.0          0.0          1.5       34.0        43.0        33.0   \n",
      "2           0.0          0.0          1.5       34.0        36.0        29.0   \n",
      "3           0.0          0.0          1.0       34.0        30.0        32.0   \n",
      "4           0.0          0.0          1.0       27.0        34.0        26.0   \n",
      "5           0.0          0.0          1.0       24.0        35.0        28.0   \n",
      "6           0.0          0.0          1.0       24.0        33.0        24.0   \n",
      "7           0.0          0.0          1.0       23.0        30.0        20.0   \n",
      "8           0.0          0.0          1.0       23.0        26.0        34.0   \n",
      "9           0.0          0.0          1.0       15.0        27.0        40.0   \n",
      "10          0.0          0.0          1.0       16.0        24.0        29.0   \n",
      "11          0.0          0.0          1.0       21.0        32.0        22.0   \n",
      "12          0.0          0.0          1.0       27.0        35.0        28.0   \n",
      "13          0.0          0.0          2.0       32.0        34.0        27.0   \n",
      "14          0.5          0.0          2.0       27.0        36.0        25.0   \n",
      "15          0.0          0.0          1.0       26.0        33.0        23.0   \n",
      "16          0.0          0.0          1.0       19.0        22.0        17.0   \n",
      "17          0.0          0.0          2.0       15.0        17.0        20.0   \n",
      "18          0.5          0.0          2.0        9.0        20.0        15.0   \n",
      "19          0.5          0.0          2.0       10.0        23.0        15.0   \n",
      "20          0.5          0.0          1.0       13.0        27.0        63.0   \n",
      "21          0.0          0.0          1.0       14.0        30.0        36.0   \n",
      "22          0.0          0.0          1.0       25.0        28.0        32.0   \n",
      "23          0.0          0.0          1.0       21.0        33.0        14.0   \n",
      "24          0.0          0.0          1.0       19.0        23.0        10.0   \n",
      "25          0.0          0.0          1.0       19.0        26.0         9.0   \n",
      "26          0.0          0.0          1.0       16.0        23.0         5.0   \n",
      "27          0.0          0.0          1.0       11.0        16.0         6.0   \n",
      "28          0.0          0.0          1.0       11.0        10.0         3.0   \n",
      "29          0.0          0.0          1.0       10.0        12.0        10.0   \n",
      "\n",
      "    station_54  station_57  station_58  \n",
      "0          NaN         NaN         NaN  \n",
      "1         23.0        16.0        14.0  \n",
      "2         26.0        25.0        18.0  \n",
      "3         18.0        21.0        14.0  \n",
      "4         14.0        16.0        15.0  \n",
      "5         18.0        17.0        13.0  \n",
      "6         17.0        15.0        11.0  \n",
      "7         17.0        16.0        22.0  \n",
      "8         10.0        17.0        16.0  \n",
      "9         14.0        14.0        16.0  \n",
      "10        17.0        15.0        17.0  \n",
      "11        17.0        20.0        17.0  \n",
      "12        22.0        21.0        21.0  \n",
      "13        24.0        23.0        24.0  \n",
      "14        19.0        15.0        20.0  \n",
      "15        22.0        11.0        21.0  \n",
      "16        18.0        10.0        18.0  \n",
      "17        16.0         8.0        19.0  \n",
      "18        13.0        10.0        14.0  \n",
      "19        12.0        88.0        13.0  \n",
      "20        20.0       156.0        13.0  \n",
      "21         8.0        37.0         3.0  \n",
      "22         9.0        46.0        10.0  \n",
      "23        12.0        19.0        10.0  \n",
      "24        10.0         7.0        10.0  \n",
      "25         7.0         6.0         8.0  \n",
      "26         4.0         4.0         6.0  \n",
      "27         5.0         4.0         7.0  \n",
      "28         4.0         3.0         6.0  \n",
      "29         4.0         3.0         4.0  \n",
      "\n",
      "[30 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now do the actual merging based on time\n",
    "print(\"Dataframe `traffic`:\")\n",
    "print(traffic.head())\n",
    "print(\"Dataframe aq: \")\n",
    "print(aq_pivoted.head())\n",
    "merged = pd.merge(\n",
    "    traffic,\n",
    "    aq_pivoted,\n",
    "    how=\"left\",\n",
    "    left_on=[\"date\", \"hour\"],\n",
    "    right_on=[\"date\", \"hour\"]\n",
    ")\n",
    "\n",
    "# is_primary_key = not traffic.duplicated(subset=['date', 'hour']).any()\n",
    "# print(f\"Is [date, hour] a primary key? {is_primary_key}\")\n",
    "\n",
    "print(\"Final DF\")\n",
    "print(merged.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c21c5e",
   "metadata": {},
   "source": [
    "Now, we have a complete dataset (whithout the weather for now)\n",
    "\n",
    "But some values are missing (evrytime I checked, around 0.1-0.5% of the cells are NaN).\n",
    "Let us just fill them forward/backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cfbb6556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of bad (Nann) values: 3.28%\n",
      "Proportion of bad (Nann) values after filling: 0.00%\n",
      "Gaps in time axis\n",
      "418 Missing hourly entries detected, trying to fill thepm...\n",
      "No missing hours — data is continuous.\n",
      "         date  hour  traffic_z0  traffic_z1  traffic_z2  traffic_z3  \\\n",
      "0  2024-01-01     0         0.0         0.0         0.0         0.0   \n",
      "1  2024-01-01     1         0.0         0.0         0.0         1.0   \n",
      "2  2024-01-01     2         0.0         0.0         0.0         1.0   \n",
      "3  2024-01-01     3         0.0         0.0         0.0         1.0   \n",
      "4  2024-01-01     4         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   traffic_z6  traffic_z7  traffic_z8  traffic_z9  ...  traffic_z33  \\\n",
      "0         0.5         0.0         0.0         0.0  ...          0.0   \n",
      "1         0.5         0.0         1.0         1.0  ...          0.0   \n",
      "2         0.5         0.0         1.0         1.0  ...          0.0   \n",
      "3         0.5         0.0         1.0         1.0  ...          0.0   \n",
      "4         0.5         0.0         0.0         1.0  ...          0.0   \n",
      "\n",
      "   traffic_z34  traffic_z35  station_4  station_43  station_44  station_54  \\\n",
      "0          0.0          1.0       34.0        43.0        33.0        23.0   \n",
      "1          0.0          1.5       34.0        43.0        33.0        23.0   \n",
      "2          0.0          1.5       34.0        36.0        29.0        26.0   \n",
      "3          0.0          1.0       34.0        30.0        32.0        18.0   \n",
      "4          0.0          1.0       27.0        34.0        26.0        14.0   \n",
      "\n",
      "   station_57  station_58       datetime_hour  \n",
      "0        16.0        14.0 2024-01-01 00:00:00  \n",
      "1        16.0        14.0 2024-01-01 01:00:00  \n",
      "2        25.0        18.0 2024-01-01 02:00:00  \n",
      "3        21.0        14.0 2024-01-01 03:00:00  \n",
      "4        16.0        15.0 2024-01-01 04:00:00  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "bad_value_proportion = (merged.isnull().sum().sum() / merged.size) * 100\n",
    "print(f\"Proportion of bad (Nann) values: {bad_value_proportion:.2f}%\")\n",
    "merged = merged.ffill().bfill()\n",
    "bad_value_proportion = (merged.isnull().sum().sum() / merged.size) * 100\n",
    "print(f\"Proportion of bad (Nann) values after filling: {bad_value_proportion:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Detect missing rows (hours when one of the measurement systems was down)\n",
    "print(\"Gaps in time axis\")\n",
    "# Combine date and hour into a single datetime\n",
    "merged['datetime_hour'] = pd.to_datetime(merged['date'].astype(str)) + pd.to_timedelta(merged['hour'], unit='h')\n",
    "\n",
    "# Sort by time\n",
    "merged = merged.sort_values('datetime_hour').reset_index(drop=True)\n",
    "\n",
    "# Create a full range from the min to max timestamp\n",
    "full_range = pd.date_range(start=merged['datetime_hour'].min(),\n",
    "                           end=merged['datetime_hour'].max(),\n",
    "                           freq='h')\n",
    "missing_times = full_range.difference(merged['datetime_hour'])\n",
    "if len(missing_times) > 0:\n",
    "    print(f\"{len(missing_times)} Missing hourly entries detected, trying to fill thepm...\")\n",
    "    \"\"\" for t in missing_times:\n",
    "        print(\" -\", t) \"\"\"\n",
    "else:\n",
    "    print(\"No missing hours — data is continuous.\")\n",
    "\n",
    "\n",
    "# Filling the missing rows with Nan\n",
    "# Use datetime_hour as index\n",
    "merged = merged.set_index('datetime_hour')\n",
    "\n",
    "# Reindex to include all hours\n",
    "merged = merged.reindex(full_range)\n",
    "\n",
    "# Restore date/hour columns if needed\n",
    "merged['date'] = merged.index.date\n",
    "merged['hour'] = merged.index.hour\n",
    "merged = merged.reset_index(drop=True)\n",
    "\n",
    "# Then we have to fill the NaN\n",
    "for col in merged.columns:\n",
    "    # First ytr to fill with value from previous week\n",
    "    prev_week = merged[col].shift(24 * 7)  # shift by 7 days\n",
    "    merged[col] = merged[col].fillna(prev_week)\n",
    "\n",
    "    # SThen fill with value from next week\n",
    "    next_week = merged[col].shift(-24 * 7)\n",
    "    merged[col] = merged[col].fillna(next_week)\n",
    "\n",
    "merged = merged.ffill()\n",
    "\n",
    "# Re-run the check\n",
    "# Combine date and hour into a single datetime\n",
    "merged['datetime_hour'] = pd.to_datetime(merged['date'].astype(str)) + pd.to_timedelta(merged['hour'], unit='h')\n",
    "\n",
    "# Sort by time\n",
    "merged = merged.sort_values('datetime_hour').reset_index(drop=True)\n",
    "full_range = pd.date_range(start=merged['datetime_hour'].min(),\n",
    "                           end=merged['datetime_hour'].max(),\n",
    "                           freq='h')\n",
    "missing_times = full_range.difference(merged['datetime_hour'])\n",
    "if len(missing_times) > 0:\n",
    "    print(f\"{len(missing_times)} Missing hourly entries detected:\")\n",
    "    \"\"\" for t in missing_times:\n",
    "        print(\" -\", t) \"\"\"\n",
    "else:\n",
    "    print(\"No missing hours — data is continuous.\")\n",
    "\n",
    "\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccece98",
   "metadata": {},
   "source": [
    "# Adding the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea68332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached weather data from data/weather_2024-01-01_2024-11-01.json\n",
      "Weather data merged successfully!!\n",
      "         date  hour  traffic_z0  traffic_z1  traffic_z2  traffic_z3  \\\n",
      "0  2024-01-01     0         0.0         0.0         0.0         0.0   \n",
      "1  2024-01-01     1         0.0         0.0         0.0         1.0   \n",
      "2  2024-01-01     2         0.0         0.0         0.0         1.0   \n",
      "3  2024-01-01     3         0.0         0.0         0.0         1.0   \n",
      "4  2024-01-01     4         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   traffic_z6  traffic_z7  traffic_z8  traffic_z9  ...  station_4  station_43  \\\n",
      "0         0.5         0.0         0.0         0.0  ...       34.0        43.0   \n",
      "1         0.5         0.0         1.0         1.0  ...       34.0        43.0   \n",
      "2         0.5         0.0         1.0         1.0  ...       34.0        36.0   \n",
      "3         0.5         0.0         1.0         1.0  ...       34.0        30.0   \n",
      "4         0.5         0.0         0.0         1.0  ...       27.0        34.0   \n",
      "\n",
      "   station_44  station_54  station_57  station_58       datetime_hour  \\\n",
      "0        33.0        23.0        16.0        14.0 2024-01-01 00:00:00   \n",
      "1        33.0        23.0        16.0        14.0 2024-01-01 01:00:00   \n",
      "2        29.0        26.0        25.0        18.0 2024-01-01 02:00:00   \n",
      "3        32.0        18.0        21.0        14.0 2024-01-01 03:00:00   \n",
      "4        26.0        14.0        16.0        15.0 2024-01-01 04:00:00   \n",
      "\n",
      "   temperature    wind_u    wind_v  \n",
      "0          5.6 -3.230844  2.524212  \n",
      "1          5.4 -6.188619  5.379684  \n",
      "2          5.6 -5.043269  3.940233  \n",
      "3          5.9 -5.400154  3.244740  \n",
      "4          6.3 -6.138531  1.417192  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Barcelona location\n",
    "lat = 41.38\n",
    "lon = 2.18\n",
    "timezone = \"Europe/Madrid\"\n",
    "\n",
    "# Defining the time range for the request\n",
    "start = merged['datetime_hour'].min().strftime('%Y-%m-%dT%H:%M')\n",
    "end = merged['datetime_hour'].max().strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "# File to cache results\n",
    "cache_file = f\"data/weather_{start[:10]}_{end[:10]}.json\"\n",
    "\n",
    "# Fetch or load from cache\n",
    "if os.path.exists(cache_file):\n",
    "    print(f\"Using cached weather data from {cache_file}\")\n",
    "    with open(cache_file, \"r\") as f:\n",
    "        weather_data = json.load(f)\n",
    "else:\n",
    "    print(\"Fetching weather data from Open-Meteo API...\")\n",
    "    url = (\n",
    "        f\"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        f\"?latitude={lat}&longitude={lon}\"\n",
    "        f\"&start_date={start[:10]}&end_date={end[:10]}\"\n",
    "        f\"&hourly=temperature_2m,windspeed_10m,winddirection_10m\"\n",
    "        f\"&timezone={timezone}\"\n",
    "    )\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    weather_data = resp.json()\n",
    "\n",
    "    # Save to cache file\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(weather_data, f)\n",
    "    print(f\"Saved weather data to {cache_file}\")\n",
    "\n",
    "# Create the DF\n",
    "hourly = weather_data[\"hourly\"]\n",
    "weather = pd.DataFrame({\n",
    "    \"datetime_hour\": hourly[\"time\"],\n",
    "    \"temperature\": hourly[\"temperature_2m\"],\n",
    "    \"wind_speed\": hourly[\"windspeed_10m\"],\n",
    "    \"wind_direction\": hourly[\"winddirection_10m\"],\n",
    "})\n",
    "weather[\"datetime_hour\"] = pd.to_datetime(weather[\"datetime_hour\"])\n",
    "\n",
    "# Encode wind direction (degrees are circular, hence not very meaningful. We will encode both speed and direction as a horizontal vector)\n",
    "weather[\"wind_u\"] = weather[\"wind_speed\"] * np.sin(np.deg2rad(weather[\"wind_direction\"]))\n",
    "weather[\"wind_v\"] = weather[\"wind_speed\"] * np.cos(np.deg2rad(weather[\"wind_direction\"]))\n",
    "\n",
    "weather=weather.drop(['wind_speed', 'wind_direction'], axis=1)\n",
    "\n",
    "# Merge with the traffic/air quality\n",
    "merged = merged.merge(weather, how=\"left\", on=\"datetime_hour\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Weather data merged successfully!!\")\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6a30b",
   "metadata": {},
   "source": [
    "## The dataset is now complete and clean\n",
    "\n",
    "Saving it to a file named `final_dataset.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5592e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_pickle('final_dataset.pkl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
