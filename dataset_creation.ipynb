{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a9c3a4",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec0af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ab3b8",
   "metadata": {},
   "source": [
    "# Downloading data (Traffic and Air quality). \n",
    "Automatically downloads from the following dates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bd2bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_from = datetime(2024, 1, 1)\n",
    "download_until = datetime(2024, 10, 28) # Note that the days of the month do not matter (as long as they exist). This selects data with a precision of a month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff831cb",
   "metadata": {},
   "source": [
    "Then run the following to actually download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ed67fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwonloading files...\n",
      "File already exists: data/TRAMS_2024_01_Gener.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_01_Gener.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_02_Febrer.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_03_Marc.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_03_Marc.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_04_Abril.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_04_Abril.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_05_Maig.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_05_Maig.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_06_Juny.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_06_Juny.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_07_Juliol.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_07_Juliol.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_08_Agost.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_08_Agost.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_09_Setembre.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_09_Setembre.csv. Skipping download.\n",
      "File already exists: data/TRAMS_2024_10_Octubre.csv. Skipping download.\n",
      "File already exists: data/QualitatAire_2024_10_Octubre.csv. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Catalan month names\n",
    "catalan_months = {\n",
    "    1: \"Gener\",\n",
    "    2: \"Febrer\",\n",
    "    3: \"Marc\",\n",
    "    4: \"Abril\",\n",
    "    5: \"Maig\",\n",
    "    6: \"Juny\",\n",
    "    7: \"Juliol\",\n",
    "    8: \"Agost\",\n",
    "    9: \"Setembre\",\n",
    "    10: \"Octubre\",\n",
    "    11: \"Novembre\",\n",
    "    12: \"Desembre\",\n",
    "}\n",
    "\n",
    "# Trams transit relacio (relation between ids and locations)\n",
    "trams_relacio_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/1090983a-1c40-4609-8620-14ad49aae3ab/resource/1d6c814c-70ef-4147-aa16-a49ddb952f72/download/transit_relacio_trams.csv\"\n",
    "trams_relacio_path = \"./data/transit_relacio_trams.csv\"\n",
    "\n",
    "# Air quality stations info (including lat and long)\n",
    "# (The stations are unchanged since 2023 so downloading only the 2025 version is enough)\n",
    "air_stations_info_url = \"https://opendata-ajuntament.barcelona.cat/data/dataset/4dff88b1-151b-48db-91c2-45007cd5d07a/resource/d1aa40d7-66f9-451b-85f8-955b765fdc2f/download/2025_qualitat_aire_estacions.csv\"\n",
    "air_stations_info_path = \"./data/air_stations_info.csv\"\n",
    "\n",
    "def generate_urls(download_from, download_until):\n",
    "    urls = []\n",
    "    current_date = datetime(download_from.year, download_from.month, 1)\n",
    "    end_date = datetime(download_until.year, download_until.month, 1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        month_name = catalan_months[month]\n",
    "\n",
    "        # Generate URLs for both datasets\n",
    "        tram_url = f\"https://opendata-ajuntament.barcelona.cat/resources/auto/transit/{year}_{month:02d}_{month_name}_TRAMS_TRAMS.csv\"\n",
    "        aire_url = f\"https://opendata-ajuntament.barcelona.cat/resources/bcn/QualitatAire/{year}_{month:02d}_{month_name}_qualitat_aire_BCN.csv\"\n",
    "\n",
    "        tram_filename = f\"data/TRAMS_{year}_{month:02d}_{month_name}.csv\"\n",
    "        aire_filename = f\"data/QualitatAire_{year}_{month:02d}_{month_name}.csv\"\n",
    "\n",
    "        urls.append((tram_url, tram_filename))\n",
    "        urls.append((aire_url, aire_filename))\n",
    "\n",
    "        # Move to the next month\n",
    "        if current_date.month == 12:\n",
    "            current_date = datetime(current_date.year + 1, 1, 1)\n",
    "        else:\n",
    "            current_date = datetime(current_date.year, current_date.month + 1, 1)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Create a \"data\" folder:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "urls = generate_urls(download_from, download_until)\n",
    "\n",
    "print(\"Dwonloading files...\")\n",
    "for url, filename in urls:\n",
    "    download_file(url, filename)\n",
    "\n",
    "\n",
    "# Then download the TRAMS relacio file\n",
    "if not os.path.exists(trams_relacio_path):\n",
    "    download_file(trams_relacio_url, trams_relacio_path)\n",
    "\n",
    "\n",
    "# And the air stations info \n",
    "if not os.path.exists(air_stations_info_path):\n",
    "    download_file(air_stations_info_url, air_stations_info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aec697",
   "metadata": {},
   "source": [
    "# Data treatment and cleaning\n",
    "\n",
    "\n",
    "## Creating a merged dataset\n",
    "\n",
    "### Combining all the air quality files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5e437d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CODI_PROVINCIA  PROVINCIA  CODI_MUNICIPI   MUNICIPI  ESTACIO  \\\n",
      "62               8  Barcelona             19  Barcelona        4   \n",
      "63               8  Barcelona             19  Barcelona        4   \n",
      "64               8  Barcelona             19  Barcelona        4   \n",
      "65               8  Barcelona             19  Barcelona        4   \n",
      "66               8  Barcelona             19  Barcelona        4   \n",
      "\n",
      "    CODI_CONTAMINANT   ANY  MES  DIA   H01  ...   H20  V20   H21  V21   H22  \\\n",
      "62                10  2024    1    1  34.0  ...  13.0    V  14.0    V  25.0   \n",
      "63                10  2024    1    2  19.0  ...  36.0    V  37.0    V  29.0   \n",
      "64                10  2024    1    3  30.0  ...  29.0    V  19.0    V  15.0   \n",
      "65                10  2024    1    4  25.0  ...  30.0    V  28.0    V  27.0   \n",
      "66                10  2024    1    5  26.0  ...  13.0    V   7.0    V   5.0   \n",
      "\n",
      "    V22   H23  V23   H24  V24  \n",
      "62    V  21.0    V  19.0    V  \n",
      "63    V  29.0    V  24.0    V  \n",
      "64    V  19.0    V  19.0    V  \n",
      "65    V  33.0    V  35.0    V  \n",
      "66    V   5.0    V   5.0    V  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" ## Now merge the air_quality measurements and the location\\n\\nair_quality = air_quality_combined.merge(\\n    right=air_station_locations,\\n    left_on='ESTACIO',\\n    right_on='Estacio',\\n    how='left'\\n).drop(columns=['Estacio'])\\n\\nprint(air_quality.head()) \""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "air_quality_files = [filename for url, filename in urls if \"QualitatAire\" in filename]\n",
    "#print(air_quality_files)\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "air_quality_dfs = [pd.read_csv(file) for file in air_quality_files]\n",
    "# Combine them into a single dataframe\n",
    "air_quality = pd.concat(air_quality_dfs, ignore_index=True)\n",
    "#print(air_quality_combined.head())\n",
    "\n",
    "\n",
    "\n",
    "# Only keep PM10 DATA\n",
    "air_quality = air_quality[air_quality[\"CODI_CONTAMINANT\"].isin([10, 110])]\n",
    "print(air_quality.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "air_station_info = pd.read_csv(air_stations_info_path)\n",
    "# Only keep location data for each station\n",
    "air_station_locations = air_station_info[['Estacio', 'Latitud', 'Longitud']].drop_duplicates(subset=['Estacio'])\n",
    "#print(air_station_locations.head(n=20))\n",
    "\n",
    "\"\"\" ## Now merge the air_quality measurements and the location\n",
    "\n",
    "air_quality = air_quality_combined.merge(\n",
    "    right=air_station_locations,\n",
    "    left_on='ESTACIO',\n",
    "    right_on='Estacio',\n",
    "    how='left'\n",
    ").drop(columns=['Estacio'])\n",
    "\n",
    "print(air_quality.head()) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d460",
   "metadata": {},
   "source": [
    "### Combining traffic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9908b47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual  estatPrevist\n",
      "0       1  20240101000552            0             0\n",
      "1       2  20240101000552            0             0\n",
      "2       3  20240101000552            0             0\n",
      "3       4  20240101000552            0             0\n",
      "4       5  20240101000552            0             0\n",
      "RESULT FINA\n",
      "   idTram            data  estatActual        lat       lon\n",
      "0      27  20240101000552            2  41.382080  2.114583\n",
      "1      62  20240101000552            1  41.396526  2.146327\n",
      "2      67  20240101000552            1  41.408015  2.138228\n",
      "3      69  20240101000552            1  41.402681  2.144603\n",
      "4      97  20240101000552            2  41.401097  2.136411\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of measurements\n",
    "\n",
    "traffic_files = [filename for url, filename in urls if \"TRAMS\" in filename]\n",
    "\n",
    "# Covnert to pandas dataframes\n",
    "traffic_dfs = [pd.read_csv(file) for file in traffic_files]\n",
    "#print(traffic_dfs[0].head())\n",
    "\n",
    "# Combine them into a single dataframe\n",
    "traffic_combined = pd.concat(traffic_dfs, ignore_index=True)\n",
    "print(traffic_combined.head())\n",
    "\n",
    "\n",
    "# Drop the rows with no data (estatActual=0)\n",
    "traffic_combined = traffic_combined.query(\"estatActual != 0\")\n",
    "\n",
    "\n",
    "## Create dataframe of station info\n",
    "trams_info = pd.read_csv(trams_relacio_path)\n",
    "# Only keep location data for each station\n",
    "trams_locations = trams_info[['Tram', 'Coordenades']]\n",
    "\n",
    "# Only keep one location point per section (tram=section)\n",
    "def mean_coordinate(row):\n",
    "    # print(\"entering function mean\")\n",
    "    # print(row)\n",
    "    coord_text=row['Coordenades']\n",
    "    numbers=coord_text.split(',')\n",
    "    assert len(numbers) % 2 == 0\n",
    "    lons = [float(x) for x in numbers[::2]]\n",
    "    lats = [float(x) for x in numbers[1::2]]\n",
    "    # print(\"infos\")\n",
    "    # print(lats)\n",
    "    # print(type(lats[0]))\n",
    "    lat = np.mean(lats)\n",
    "    lon = np.mean(lons)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "trams_locations [['lat' , 'lon']] = trams_locations.apply(mean_coordinate, axis=1)\n",
    "#print(trams_locations.head())\n",
    "\n",
    "\n",
    "## Now merge the traffic measurements and the location\n",
    "traffic = traffic_combined.merge(\n",
    "    right=trams_locations,\n",
    "    left_on='idTram',\n",
    "    right_on='Tram',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "traffic = traffic.drop(['Coordenades', 'Tram', 'estatPrevist'], axis=1)\n",
    "\n",
    "print(\"RESULT FINA\")\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364615f",
   "metadata": {},
   "source": [
    "## Dataset issue\n",
    "\n",
    "For some reason, the city does not provide information for the 'trams' with id greater than 527. In the `traffic` dataframe, there are some records with idTram between 535 and 539. They don't have a latitude/longitude. Let us drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b751bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = traffic.dropna(subset=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6d93",
   "metadata": {},
   "source": [
    "# Defining a traffic grid\n",
    "\n",
    "Now, `traffic` contains the state (from 0-no car to 6-congestioned) of plenty of coordinates in Barcelona. But the density is not homogeneous, hence i will cut the city in a grid and compute an average value for traffic congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3eb04e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idTram            data  estatActual        lat       lon  zone\n",
      "0      27  20240101000552            2  41.382080  2.114583     3\n",
      "1      62  20240101000552            1  41.396526  2.146327     4\n",
      "2      67  20240101000552            1  41.408015  2.138228     3\n",
      "3      69  20240101000552            1  41.402681  2.144603     4\n",
      "4      97  20240101000552            2  41.401097  2.136411     3\n"
     ]
    }
   ],
   "source": [
    "# Getting the boundaries of the traffic information\n",
    "minlat = traffic['lat'].min()\n",
    "maxlat = traffic['lat'].max()\n",
    "minlon = traffic['lon'].min()\n",
    "maxlon = traffic['lon'].max()\n",
    "\n",
    "#TODO add prints of the distance of the box here\n",
    "\n",
    "\n",
    "nb_horizontal = 3\n",
    "nb_vertical = 3\n",
    "\n",
    "vertical_step = (maxlat - minlat) / nb_vertical\n",
    "horizontal_step = (maxlon - minlon) / nb_horizontal\n",
    "\n",
    "\n",
    "# Create new datafram from traffic\n",
    "\n",
    "\"\"\" def getZone(row):\n",
    "    lat = (row['lat'] - minlat) // vertical_step\n",
    "    lon = (row['lon'] - minlon) // horizontal_step\n",
    "    zone_number = (lat * nb_horizontal) + lon\n",
    "\n",
    "\n",
    "traffic['zone'] = traffic.apply(getZone, axis=1) \"\"\"\n",
    "# BETTER WAY (vectorized)\n",
    "# Create nb_vertical * nb_horizontal zones and assign every row a zone:\n",
    "lat_idx = np.clip(np.floor((traffic['lat'] - minlat) / vertical_step).astype(int), 0, nb_vertical - 1)\n",
    "lon_idx = np.clip(np.floor((traffic['lon'] - minlon) / horizontal_step).astype(int), 0, nb_horizontal - 1)\n",
    "traffic['zone'] = lat_idx * nb_horizontal + lon_idx\n",
    "\n",
    "assert traffic['zone'].max() <= nb_horizontal * nb_vertical\n",
    "\n",
    "# Drop lat/lon as we will now use the zone\n",
    "traffic.drop(['lat', 'lon'], axis=1)\n",
    "\n",
    "\n",
    "print(traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ecc1a",
   "metadata": {},
   "source": [
    "# Merging all the data\n",
    "We are then going to merge both of the dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615fd52",
   "metadata": {},
   "source": [
    "## Modification of the `traffic` dataframe\n",
    "Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour. Then we will melt the dataframe so that [hour, date] is a primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "88fdc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone        date  hour  traffic_z0  traffic_z1  traffic_z3  traffic_z4  \\\n",
      "0     2024-01-01     0         2.0         2.0         1.0         2.0   \n",
      "1     2024-01-01     1         3.0         2.0         2.0         2.0   \n",
      "2     2024-01-01     2         2.0         2.0         2.0         2.0   \n",
      "3     2024-01-01     3         2.0         2.0         2.0         2.0   \n",
      "4     2024-01-01     4         3.0         1.0         1.0         2.0   \n",
      "\n",
      "zone  traffic_z5  traffic_z6  traffic_z7  traffic_z8  \n",
      "0            2.0         1.0         1.0         1.0  \n",
      "1            2.0         2.0         2.0         2.0  \n",
      "2            2.0         2.0         2.0         2.0  \n",
      "3            2.0         2.0         2.0         2.0  \n",
      "4            2.0         1.0         1.0         1.0  \n"
     ]
    }
   ],
   "source": [
    "# Covnert the 'data' (date) column of traffic dataframe\n",
    "traffic[\"datetime\"] = pd.to_datetime(traffic[\"data\"], format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Traffic measurements are made more or less 10 minutes around o'clock hours. Then we will round the time to the nearest hour \n",
    "traffic[\"datetime_hour\"] = traffic[\"datetime\"].dt.round(\"h\")\n",
    "traffic[\"date\"] = traffic[\"datetime_hour\"].dt.date\n",
    "traffic[\"hour\"] = traffic[\"datetime_hour\"].dt.hour\n",
    "\n",
    "\n",
    "# Use 85th percentile instead of the median (50th percentile)\n",
    "def percentile_85(x):\n",
    "    return np.percentile(x, 85)\n",
    "\n",
    "# Pivoting\n",
    "traffic = traffic.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['zone'],\n",
    "    values='estatActual',\n",
    "    aggfunc=percentile_85\n",
    ").add_prefix('traffic_z').reset_index()\n",
    "print(traffic.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bf4c1",
   "metadata": {},
   "source": [
    "## Modification of the `air_quality` dataframe\n",
    "The hours are currently in the columns, we are going to melt the dataframe to have a long one with dates in the rows.\n",
    "\n",
    "We also want [hour,date] to be a primary key so that merging both dataframes does not explodes RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b4a5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  hour  station_4  station_43  station_44  station_54  \\\n",
      "0    2024-01-01     1       34.0        43.0        33.0        23.0   \n",
      "1    2024-01-01     2       34.0        36.0        29.0        26.0   \n",
      "2    2024-01-01     3       34.0        30.0        32.0        18.0   \n",
      "3    2024-01-01     4       27.0        34.0        26.0        14.0   \n",
      "4    2024-01-01     5       24.0        35.0        28.0        18.0   \n",
      "..          ...   ...        ...         ...         ...         ...   \n",
      "195  2024-01-09     4       17.0        11.0         6.0         8.0   \n",
      "196  2024-01-09     5       13.0        10.0         7.0         6.0   \n",
      "197  2024-01-09     6       11.0        10.0         7.0         5.0   \n",
      "198  2024-01-09     7       13.0        12.0        12.0        10.0   \n",
      "199  2024-01-09     8       33.0        21.0        24.0        14.0   \n",
      "\n",
      "     station_57  station_58  \n",
      "0          16.0        14.0  \n",
      "1          25.0        18.0  \n",
      "2          21.0        14.0  \n",
      "3          16.0        15.0  \n",
      "4          17.0        13.0  \n",
      "..          ...         ...  \n",
      "195         5.0         8.0  \n",
      "196         5.0        10.0  \n",
      "197         6.0        12.0  \n",
      "198         8.0        11.0  \n",
      "199        11.0        15.0  \n",
      "\n",
      "[200 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the hours columns name\n",
    "hour_cols = [col for col in air_quality.columns if col.startswith(\"H\") and len(col) == 3]\n",
    "\n",
    "# Then use pandas melt method to reshape the DF\n",
    "\n",
    "aq_melted = air_quality.melt(\n",
    "    # Columns left untouched\n",
    "    id_vars=[\"CODI_PROVINCIA\", \"PROVINCIA\", \"CODI_MUNICIPI\", \"MUNICIPI\", \"ESTACIO\", \"ANY\", \"MES\", \"DIA\"], # IMPORTANT: i removed CODI_CONTAMINANT because i only kept PM10 temporarily\n",
    "\n",
    "    #Columns to unpivot\n",
    "    value_vars=hour_cols,\n",
    "\n",
    "    #Name of the new column that will store the unpivoted columns names (H01 etc.)\n",
    "    var_name=\"hour_str\",\n",
    "\n",
    "    #Name of the actual value column\n",
    "    value_name=\"pollution_value\"\n",
    ")\n",
    "    \n",
    "# Clean the hour (H01->1)\n",
    "aq_melted[\"hour\"] = aq_melted[\"hour_str\"].str.extract(r\"H(\\d{2})\").astype(\"int32\")\n",
    "\n",
    "\n",
    "\n",
    "# Create the actual date column (and translate column names)\n",
    "aq_melted[\"date\"] = pd.to_datetime(\n",
    "    aq_melted[[\"ANY\", \"MES\", \"DIA\"]].rename(columns={\"ANY\": \"year\", \"MES\": \"month\", \"DIA\": \"day\"})\n",
    ").dt.date\n",
    "\n",
    "#Shift the H24 columns to hour 0 of the next day:\n",
    "mask_24 = aq_melted[\"hour\"] == 24\n",
    "aq_melted.loc[mask_24, \"hour\"] = 0\n",
    "aq_melted.loc[mask_24, \"date\"] = pd.to_datetime(aq_melted.loc[mask_24, \"date\"]) + pd.Timedelta(days=1)\n",
    "aq_melted[\"date\"] = pd.to_datetime(aq_melted[\"date\"]).dt.date\n",
    "\n",
    "\n",
    "# Pivot (CODI_CONTAMINANT,ESTACIO) so that [hour, date] is primary key\n",
    "aq_pivoted = aq_melted.pivot_table(\n",
    "    index=['date', 'hour'],\n",
    "    columns=['ESTACIO'], # IMPORTANT: here i also remove CODI_CONTAMINANT since there is only one.\n",
    "    values='pollution_value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Simple flattening (for more meaningful column names)\n",
    "\"\"\" aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'pollutant_{col[0]}_station_{col[1]}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "] \"\"\"\n",
    "aq_pivoted.columns = ['date', 'hour'] + [\n",
    "    f'station_{col}' \n",
    "    for col in aq_pivoted.columns[2:]\n",
    "]\n",
    "\n",
    "print(aq_pivoted.head(200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b1f32",
   "metadata": {},
   "source": [
    "## Dataframes cleaning\n",
    "Before merging the datasets, we have to ensure the hourly time is continous. We also should monitor and fill the NaN values.\n",
    "It is important to do so before merging, to minimise the effect on the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cfbb6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    bad_value_proportion = (df.isnull().sum().sum() / df.size) * 100\n",
    "    print(f\"Proportion of bad (Nan) values: {bad_value_proportion:.2f}%\")\n",
    "    df = df.ffill().bfill()\n",
    "    bad_value_proportion = (df.isnull().sum().sum() / df.size) * 100\n",
    "    print(f\"Proportion of bad (Nan) values after filling: {bad_value_proportion:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "    # Detect missing rows (hours when one of the measurement systems was down)\n",
    "    print(\"Gaps in time axis\")\n",
    "    # Combine date and hour into a single datetime\n",
    "    df['datetime_hour'] = pd.to_datetime(df['date'].astype(str)) + pd.to_timedelta(df['hour'], unit='h')\n",
    "\n",
    "    # Sort by time\n",
    "    df = df.sort_values('datetime_hour').reset_index(drop=True)\n",
    "\n",
    "    # Create a full range from the min to max timestamp\n",
    "    full_range = pd.date_range(start=df['datetime_hour'].min(),\n",
    "                            end=df['datetime_hour'].max(),\n",
    "                            freq='h')\n",
    "    missing_times = full_range.difference(df['datetime_hour'])\n",
    "    if len(missing_times) > 0:\n",
    "        print(f\"{len(missing_times)} Missing hourly entries detected, trying to fill them...\")\n",
    "        \"\"\" for t in missing_times:\n",
    "            print(\" -\", t) \"\"\"\n",
    "    else:\n",
    "        print(\"No missing hours — data is continuous.\")\n",
    "\n",
    "\n",
    "    # Filling the missing rows with Nan\n",
    "    # Use datetime_hour as index\n",
    "    df = df.set_index('datetime_hour')\n",
    "\n",
    "    # Reindex to include all hours\n",
    "    df = df.reindex(full_range)\n",
    "\n",
    "    # Restore date/hour columns if needed\n",
    "    df['date'] = df.index.date\n",
    "    df['hour'] = df.index.hour\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Then we have to fill the NaN\n",
    "    for col in df.columns:\n",
    "        # First ytr to fill with value from previous week\n",
    "        prev_week = df[col].shift(24 * 7)  # shift by 7 days\n",
    "        df[col] = df[col].fillna(prev_week)\n",
    "\n",
    "        # SThen fill with value from next week\n",
    "        next_week = df[col].shift(-24 * 7)\n",
    "        df[col] = df[col].fillna(next_week)\n",
    "\n",
    "    df = df.ffill()\n",
    "\n",
    "    # Re-run the check\n",
    "    # Combine date and hour into a single datetime\n",
    "    df['datetime_hour'] = pd.to_datetime(df['date'].astype(str)) + pd.to_timedelta(df['hour'], unit='h')\n",
    "\n",
    "    # Sort by time\n",
    "    df = df.sort_values('datetime_hour').reset_index(drop=True)\n",
    "    full_range = pd.date_range(start=df['datetime_hour'].min(),\n",
    "                            end=df['datetime_hour'].max(),\n",
    "                            freq='h')\n",
    "    missing_times = full_range.difference(df['datetime_hour'])\n",
    "    if len(missing_times) > 0:\n",
    "        print(f\"{len(missing_times)} Missing hourly entries detected:\")\n",
    "        \"\"\" for t in missing_times:\n",
    "            print(\" -\", t) \"\"\"\n",
    "    else:\n",
    "        print(\"No missing hours — data is continuous.\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b3296565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and filling 'traffic'...\n",
      "Proportion of bad (Nan) values: 0.02%\n",
      "Proportion of bad (Nan) values after filling: 0.00%\n",
      "Gaps in time axis\n",
      "418 Missing hourly entries detected, trying to fill them...\n",
      "No missing hours — data is continuous.\n",
      "Cleaning and filling 'aq_pivoted'...\n",
      "Proportion of bad (Nan) values: 14.75%\n",
      "Proportion of bad (Nan) values after filling: 0.00%\n",
      "Gaps in time axis\n",
      "1 Missing hourly entries detected, trying to fill them...\n",
      "No missing hours — data is continuous.\n"
     ]
    }
   ],
   "source": [
    "# Perform the cleaning on the two dataframes\n",
    "print(\"Cleaning and filling 'traffic'...\")\n",
    "traffic = fill_missing_values(traffic)\n",
    "\n",
    "print(\"Cleaning and filling 'aq_pivoted'...\")\n",
    "aq_pivoted = fill_missing_values(aq_pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c08b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and filling 'merged'...\n",
      "Proportion of bad (Nan) values: 0.01%\n",
      "Proportion of bad (Nan) values after filling: 0.00%\n",
      "Gaps in time axis\n",
      "No missing hours — data is continuous.\n",
      "No missing hours — data is continuous.\n",
      "         date  hour  traffic_z0  traffic_z1  traffic_z3  traffic_z4  \\\n",
      "0  2024-01-01     0         2.0         2.0         1.0         2.0   \n",
      "1  2024-01-01     1         3.0         2.0         2.0         2.0   \n",
      "2  2024-01-01     2         2.0         2.0         2.0         2.0   \n",
      "3  2024-01-01     3         2.0         2.0         2.0         2.0   \n",
      "4  2024-01-01     4         3.0         1.0         1.0         2.0   \n",
      "\n",
      "   traffic_z5  traffic_z6  traffic_z7  traffic_z8     datetime_hour_x  \\\n",
      "0         2.0         1.0         1.0         1.0 2024-01-01 00:00:00   \n",
      "1         2.0         2.0         2.0         2.0 2024-01-01 01:00:00   \n",
      "2         2.0         2.0         2.0         2.0 2024-01-01 02:00:00   \n",
      "3         2.0         2.0         2.0         2.0 2024-01-01 03:00:00   \n",
      "4         2.0         1.0         1.0         1.0 2024-01-01 04:00:00   \n",
      "\n",
      "   station_4  station_43  station_44  station_54  station_57  station_58  \\\n",
      "0       34.0        43.0        33.0        23.0        16.0        14.0   \n",
      "1       34.0        43.0        33.0        23.0        16.0        14.0   \n",
      "2       34.0        36.0        29.0        26.0        25.0        18.0   \n",
      "3       34.0        30.0        32.0        18.0        21.0        14.0   \n",
      "4       27.0        34.0        26.0        14.0        16.0        15.0   \n",
      "\n",
      "      datetime_hour_y       datetime_hour  \n",
      "0 2024-01-01 01:00:00 2024-01-01 00:00:00  \n",
      "1 2024-01-01 01:00:00 2024-01-01 01:00:00  \n",
      "2 2024-01-01 02:00:00 2024-01-01 02:00:00  \n",
      "3 2024-01-01 03:00:00 2024-01-01 03:00:00  \n",
      "4 2024-01-01 04:00:00 2024-01-01 04:00:00  \n",
      "Final DF\n",
      "          date  hour  traffic_z0  traffic_z1  traffic_z3  traffic_z4  \\\n",
      "0   2024-01-01     0        2.00         2.0        1.00         2.0   \n",
      "1   2024-01-01     1        3.00         2.0        2.00         2.0   \n",
      "2   2024-01-01     2        2.00         2.0        2.00         2.0   \n",
      "3   2024-01-01     3        2.00         2.0        2.00         2.0   \n",
      "4   2024-01-01     4        3.00         1.0        1.00         2.0   \n",
      "5   2024-01-01     5        1.75         1.0        1.00         2.0   \n",
      "6   2024-01-01     6        2.00         2.0        1.00         2.0   \n",
      "7   2024-01-01     7        2.00         2.0        2.00         2.0   \n",
      "8   2024-01-01     8        2.00         2.0        2.00         2.0   \n",
      "9   2024-01-01     9        2.00         2.0        2.00         2.0   \n",
      "10  2024-01-01    10        2.00         2.0        1.00         2.0   \n",
      "11  2024-01-01    11        2.00         2.0        2.00         2.0   \n",
      "12  2024-01-01    12        2.00         2.0        2.00         2.0   \n",
      "13  2024-01-01    13        2.00         2.0        2.00         2.0   \n",
      "14  2024-01-01    14        2.00         2.0        2.00         2.0   \n",
      "15  2024-01-01    15        2.00         2.0        2.00         2.0   \n",
      "16  2024-01-01    16        2.00         2.0        2.00         2.0   \n",
      "17  2024-01-01    17        2.00         2.0        2.00         2.0   \n",
      "18  2024-01-01    18        2.00         2.0        2.00         2.0   \n",
      "19  2024-01-01    19        2.00         2.0        2.00         2.0   \n",
      "20  2024-01-01    20        3.00         2.0        2.00         2.0   \n",
      "21  2024-01-01    21        2.00         2.0        2.00         2.0   \n",
      "22  2024-01-01    22        3.00         2.0        2.00         2.0   \n",
      "23  2024-01-01    23        2.00         1.4        1.00         2.0   \n",
      "24  2024-01-02     0        2.00         1.0        1.00         2.0   \n",
      "25  2024-01-02     1        2.00         1.0        1.00         6.0   \n",
      "26  2024-01-02     2        2.00         2.0        1.65         6.0   \n",
      "27  2024-01-02     3        2.00         1.0        1.00         6.0   \n",
      "28  2024-01-02     4        2.00         1.0        2.00         6.0   \n",
      "29  2024-01-02     5        2.00         1.0        1.00         6.0   \n",
      "\n",
      "    traffic_z5  traffic_z6  traffic_z7  traffic_z8     datetime_hour_x  \\\n",
      "0          2.0         1.0        1.00         1.0 2024-01-01 00:00:00   \n",
      "1          2.0         2.0        2.00         2.0 2024-01-01 01:00:00   \n",
      "2          2.0         2.0        2.00         2.0 2024-01-01 02:00:00   \n",
      "3          2.0         2.0        2.00         2.0 2024-01-01 03:00:00   \n",
      "4          2.0         1.0        1.00         1.0 2024-01-01 04:00:00   \n",
      "5          2.0         1.0        1.00         1.0 2024-01-01 05:00:00   \n",
      "6          2.0         1.0        2.00         1.0 2024-01-01 06:00:00   \n",
      "7          2.0         1.0        1.00         2.0 2024-01-01 07:00:00   \n",
      "8          2.0         1.0        2.00         1.0 2024-01-01 08:00:00   \n",
      "9          2.0         1.0        1.00         1.0 2024-01-01 09:00:00   \n",
      "10         2.0         1.0        1.45         1.0 2024-01-01 10:00:00   \n",
      "11         2.0         1.0        1.00         2.0 2024-01-01 11:00:00   \n",
      "12         2.0         2.0        2.00         2.0 2024-01-01 12:00:00   \n",
      "13         2.0         2.0        2.00         2.0 2024-01-01 13:00:00   \n",
      "14         3.0         2.0        2.00         2.0 2024-01-01 14:00:00   \n",
      "15         2.0         2.0        2.00         2.0 2024-01-01 15:00:00   \n",
      "16         2.0         2.0        2.00         2.0 2024-01-01 16:00:00   \n",
      "17         2.0         2.0        2.00         2.0 2024-01-01 17:00:00   \n",
      "18         2.0         2.0        2.00         2.0 2024-01-01 18:00:00   \n",
      "19         2.0         2.0        2.00         2.0 2024-01-01 19:00:00   \n",
      "20         2.0         2.0        2.00         2.0 2024-01-01 20:00:00   \n",
      "21         2.0         2.0        2.00         2.0 2024-01-01 21:00:00   \n",
      "22         2.0         2.0        2.00         2.0 2024-01-01 22:00:00   \n",
      "23         2.0         1.0        1.00         1.0 2024-01-01 23:00:00   \n",
      "24         2.0         1.0        1.00         1.0 2024-01-02 00:00:00   \n",
      "25         2.0         1.0        1.00         1.0 2024-01-02 01:00:00   \n",
      "26         2.0         1.0        1.00         1.0 2024-01-02 02:00:00   \n",
      "27         2.0         1.0        1.00         1.0 2024-01-02 03:00:00   \n",
      "28         2.0         1.0        1.00         1.0 2024-01-02 04:00:00   \n",
      "29         2.0         1.0        1.00         1.0 2024-01-02 05:00:00   \n",
      "\n",
      "    station_4  station_43  station_44  station_54  station_57  station_58  \\\n",
      "0        34.0        43.0        33.0        23.0        16.0        14.0   \n",
      "1        34.0        43.0        33.0        23.0        16.0        14.0   \n",
      "2        34.0        36.0        29.0        26.0        25.0        18.0   \n",
      "3        34.0        30.0        32.0        18.0        21.0        14.0   \n",
      "4        27.0        34.0        26.0        14.0        16.0        15.0   \n",
      "5        24.0        35.0        28.0        18.0        17.0        13.0   \n",
      "6        24.0        33.0        24.0        17.0        15.0        11.0   \n",
      "7        23.0        30.0        20.0        17.0        16.0        22.0   \n",
      "8        23.0        26.0        34.0        10.0        17.0        16.0   \n",
      "9        15.0        27.0        40.0        14.0        14.0        16.0   \n",
      "10       16.0        24.0        29.0        17.0        15.0        17.0   \n",
      "11       21.0        32.0        22.0        17.0        20.0        17.0   \n",
      "12       27.0        35.0        28.0        22.0        21.0        21.0   \n",
      "13       32.0        34.0        27.0        24.0        23.0        24.0   \n",
      "14       27.0        36.0        25.0        19.0        15.0        20.0   \n",
      "15       26.0        33.0        23.0        22.0        11.0        21.0   \n",
      "16       19.0        22.0        17.0        18.0        10.0        18.0   \n",
      "17       15.0        17.0        20.0        16.0         8.0        19.0   \n",
      "18        9.0        20.0        15.0        13.0        10.0        14.0   \n",
      "19       10.0        23.0        15.0        12.0        88.0        13.0   \n",
      "20       13.0        27.0        63.0        20.0       156.0        13.0   \n",
      "21       14.0        30.0        36.0         8.0        37.0         3.0   \n",
      "22       25.0        28.0        32.0         9.0        46.0        10.0   \n",
      "23       21.0        33.0        14.0        12.0        19.0        10.0   \n",
      "24       19.0        23.0        10.0        10.0         7.0        10.0   \n",
      "25       19.0        26.0         9.0         7.0         6.0         8.0   \n",
      "26       16.0        23.0         5.0         4.0         4.0         6.0   \n",
      "27       11.0        16.0         6.0         5.0         4.0         7.0   \n",
      "28       13.0        12.0         7.0         6.0         3.0         7.0   \n",
      "29       11.0        10.0         3.0         4.0         3.0         6.0   \n",
      "\n",
      "       datetime_hour_y       datetime_hour  \n",
      "0  2024-01-01 01:00:00 2024-01-01 00:00:00  \n",
      "1  2024-01-01 01:00:00 2024-01-01 01:00:00  \n",
      "2  2024-01-01 02:00:00 2024-01-01 02:00:00  \n",
      "3  2024-01-01 03:00:00 2024-01-01 03:00:00  \n",
      "4  2024-01-01 04:00:00 2024-01-01 04:00:00  \n",
      "5  2024-01-01 05:00:00 2024-01-01 05:00:00  \n",
      "6  2024-01-01 06:00:00 2024-01-01 06:00:00  \n",
      "7  2024-01-01 07:00:00 2024-01-01 07:00:00  \n",
      "8  2024-01-01 08:00:00 2024-01-01 08:00:00  \n",
      "9  2024-01-01 09:00:00 2024-01-01 09:00:00  \n",
      "10 2024-01-01 10:00:00 2024-01-01 10:00:00  \n",
      "11 2024-01-01 11:00:00 2024-01-01 11:00:00  \n",
      "12 2024-01-01 12:00:00 2024-01-01 12:00:00  \n",
      "13 2024-01-01 13:00:00 2024-01-01 13:00:00  \n",
      "14 2024-01-01 14:00:00 2024-01-01 14:00:00  \n",
      "15 2024-01-01 15:00:00 2024-01-01 15:00:00  \n",
      "16 2024-01-01 16:00:00 2024-01-01 16:00:00  \n",
      "17 2024-01-01 17:00:00 2024-01-01 17:00:00  \n",
      "18 2024-01-01 18:00:00 2024-01-01 18:00:00  \n",
      "19 2024-01-01 19:00:00 2024-01-01 19:00:00  \n",
      "20 2024-01-01 20:00:00 2024-01-01 20:00:00  \n",
      "21 2024-01-01 21:00:00 2024-01-01 21:00:00  \n",
      "22 2024-01-01 22:00:00 2024-01-01 22:00:00  \n",
      "23 2024-01-01 23:00:00 2024-01-01 23:00:00  \n",
      "24 2024-01-02 00:00:00 2024-01-02 00:00:00  \n",
      "25 2024-01-02 01:00:00 2024-01-02 01:00:00  \n",
      "26 2024-01-02 02:00:00 2024-01-02 02:00:00  \n",
      "27 2024-01-02 03:00:00 2024-01-02 03:00:00  \n",
      "28 2024-01-02 04:00:00 2024-01-02 04:00:00  \n",
      "29 2024-01-02 05:00:00 2024-01-02 05:00:00  \n"
     ]
    }
   ],
   "source": [
    "# Now do the actual merging based on time\n",
    "\n",
    "merged = pd.merge(\n",
    "    traffic,\n",
    "    aq_pivoted,\n",
    "    how=\"left\",\n",
    "    left_on=[\"date\", \"hour\"],\n",
    "    right_on=[\"date\", \"hour\"]\n",
    ")\n",
    "\n",
    "# is_primary_key = not traffic.duplicated(subset=['date', 'hour']).any()\n",
    "# print(f\"Is [date, hour] a primary key? {is_primary_key}\")\n",
    "print(\"Cleaning and filling 'merged'...\")\n",
    "merged = fill_missing_values(merged)\n",
    "print(merged.head())\n",
    "print(\"Final DF\")\n",
    "print(merged.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c21c5e",
   "metadata": {},
   "source": [
    "Now, we have a complete dataset (whithout the weather for now)\n",
    "\n",
    "But some values are missing (evrytime I checked, around 0.1-0.5% of the cells are NaN).\n",
    "Let us just fill them forward/backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccece98",
   "metadata": {},
   "source": [
    "# Adding the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ea68332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached weather data from data/weather_2024-01-01_2024-11-01.json\n",
      "Weather data merged successfully!!\n",
      "         date  hour  traffic_z0  traffic_z1  traffic_z3  traffic_z4  \\\n",
      "0  2024-01-01     0         2.0         2.0         1.0         2.0   \n",
      "1  2024-01-01     1         3.0         2.0         2.0         2.0   \n",
      "2  2024-01-01     2         2.0         2.0         2.0         2.0   \n",
      "3  2024-01-01     3         2.0         2.0         2.0         2.0   \n",
      "4  2024-01-01     4         3.0         1.0         1.0         2.0   \n",
      "\n",
      "   traffic_z5  traffic_z6  traffic_z7  traffic_z8  ...     datetime_hour_y  \\\n",
      "0         2.0         1.0         1.0         1.0  ... 2024-01-01 01:00:00   \n",
      "1         2.0         2.0         2.0         2.0  ... 2024-01-01 01:00:00   \n",
      "2         2.0         2.0         2.0         2.0  ... 2024-01-01 02:00:00   \n",
      "3         2.0         2.0         2.0         2.0  ... 2024-01-01 03:00:00   \n",
      "4         2.0         1.0         1.0         1.0  ... 2024-01-01 04:00:00   \n",
      "\n",
      "        datetime_hour  temperature  precipitation  humidity  pressure  \\\n",
      "0 2024-01-01 00:00:00          5.6            0.0        87    1015.0   \n",
      "1 2024-01-01 01:00:00          5.4            0.0        92    1015.0   \n",
      "2 2024-01-01 02:00:00          5.6            0.0        90    1015.3   \n",
      "3 2024-01-01 03:00:00          5.9            0.0        84    1015.2   \n",
      "4 2024-01-01 04:00:00          6.3            0.0        77    1014.9   \n",
      "\n",
      "   cloud_cover is_raining    wind_u    wind_v  \n",
      "0          100          0 -3.230844  2.524212  \n",
      "1          100          0 -6.188619  5.379684  \n",
      "2          100          0 -5.043269  3.940233  \n",
      "3          100          0 -5.400154  3.244740  \n",
      "4          100          0 -6.138531  1.417192  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Barcelona location\n",
    "lat = 41.38\n",
    "lon = 2.18\n",
    "timezone = \"Europe/Madrid\"\n",
    "\n",
    "# Defining the time range for the request\n",
    "start = merged['datetime_hour'].min().strftime('%Y-%m-%dT%H:%M')\n",
    "end = merged['datetime_hour'].max().strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "# File to cache results\n",
    "cache_file = f\"data/weather_{start[:10]}_{end[:10]}.json\"\n",
    "\n",
    "# Fetch or load from cache\n",
    "if os.path.exists(cache_file):\n",
    "    print(f\"Using cached weather data from {cache_file}\")\n",
    "    with open(cache_file, \"r\") as f:\n",
    "        weather_data = json.load(f)\n",
    "else:\n",
    "    print(\"Fetching weather data from Open-Meteo API...\")\n",
    "    url = (\n",
    "        f\"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        f\"?latitude={lat}&longitude={lon}\"\n",
    "        f\"&start_date={start[:10]}&end_date={end[:10]}\"\n",
    "        f\"&hourly=temperature_2m,windspeed_10m,winddirection_10m,precipitation,relative_humidity_2m,surface_pressure,cloudcover\"\n",
    "        f\"&timezone={timezone}\"\n",
    "    )\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    weather_data = resp.json()\n",
    "\n",
    "    # Save to cache file\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(weather_data, f)\n",
    "    print(f\"Saved weather data to {cache_file}\")\n",
    "\n",
    "# Create the DF\n",
    "hourly = weather_data[\"hourly\"]\n",
    "weather = pd.DataFrame({\n",
    "    \"datetime_hour\": hourly[\"time\"],\n",
    "    \"temperature\": hourly[\"temperature_2m\"],\n",
    "    \"wind_speed\": hourly[\"windspeed_10m\"],\n",
    "    \"wind_direction\": hourly[\"winddirection_10m\"],\n",
    "    \"precipitation\": hourly[\"precipitation\"],  # mm of rain\n",
    "    \"humidity\": hourly[\"relative_humidity_2m\"],  # %\n",
    "    \"pressure\": hourly[\"surface_pressure\"],  # hPa\n",
    "    \"cloud_cover\": hourly[\"cloudcover\"],  # %\n",
    "})\n",
    "weather[\"datetime_hour\"] = pd.to_datetime(weather[\"datetime_hour\"])\n",
    "\n",
    "# Add binary rain indicator\n",
    "weather[\"is_raining\"] = (weather[\"precipitation\"] > 0).astype(int)\n",
    "\n",
    "# Encode wind direction (degrees are circular, hence not very meaningful. We will encode both speed and direction as a horizontal vector)\n",
    "weather[\"wind_u\"] = weather[\"wind_speed\"] * np.sin(np.deg2rad(weather[\"wind_direction\"]))\n",
    "weather[\"wind_v\"] = weather[\"wind_speed\"] * np.cos(np.deg2rad(weather[\"wind_direction\"]))\n",
    "\n",
    "weather = weather.drop(['wind_speed', 'wind_direction'], axis=1)\n",
    "\n",
    "# Merge with the traffic/air quality\n",
    "merged = merged.merge(weather, how=\"left\", on=\"datetime_hour\")\n",
    "\n",
    "print(\"Weather data merged successfully!!\")\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6a30b",
   "metadata": {},
   "source": [
    "## The dataset is now complete and clean\n",
    "\n",
    "Saving it to two files:\n",
    "- one named `final_dataset.pkl` for regression and unsupervised learning\n",
    "- one named `final_dataset_classification` for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d5592e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  station_4 station_43 station_44 station_54 station_57 station_58\n",
      "0    medium        bad     medium     medium       good       good\n",
      "1    medium        bad     medium     medium       good       good\n",
      "2    medium     medium     medium     medium     medium       good\n",
      "3    medium     medium     medium       good     medium       good\n",
      "4    medium     medium     medium       good       good       good\n"
     ]
    }
   ],
   "source": [
    "# Save the main one\n",
    "merged.to_pickle('final_dataset.pkl')\n",
    "\n",
    "# Convert the PM10 pollution values to good, medium, bad and then save the dataframe.\n",
    "\n",
    "# Thresholds for PM10 (in µg/m3) based on common EU PM10 air quality guidelines\n",
    "# - good: 0–20\n",
    "# - medium: 20–40\n",
    "# - bad: >40\n",
    "\n",
    "def label_pm10(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    elif value <= 20:\n",
    "        return \"good\"\n",
    "    elif value <= 40:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"bad\"\n",
    "\n",
    "# Suppose your DataFrame is named df\n",
    "# Automatically find all station columns\n",
    "station_cols = [col for col in merged.columns if \"station_\" in col]\n",
    "\n",
    "# Apply the labeling function to each station column\n",
    "merged[station_cols] = merged[station_cols].map(label_pm10)\n",
    "\n",
    "print(merged[station_cols].head())\n",
    "\n",
    "merged.to_pickle('final_dataset_classes.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
